#!/usr/bin/env python3

# HuggingFace tokenizers parallelism ê²½ê³  í•´ê²°
import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import argparse
import json
import random
import re
import traceback
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
import numpy as np
from pathlib import Path
import librosa
import soundfile as sf
from tqdm import tqdm
import wandb
from datetime import datetime

# Import our custom modules
from pipeline import TextToAudioProcessingPipeline
from dynamic_pipeline_factory import DynamicPipelineFactory
from encoder.text_encoder import CLAPTextEncoder  # CLAP ëª¨ë“ˆ import

class PresetDataset(Dataset):
    """ì˜¤ë””ì˜¤ ì´í™íŠ¸ í”„ë¦¬ì…‹ ë°ì´í„°ì…‹"""
    
    def __init__(self, descriptions, audio_dataset_path, use_fine_tuned_presets=False, 
                 fine_preset_path=None, sample_rate=44100, audio_length=5.0):
        self.descriptions = descriptions
        self.audio_dataset_path = audio_dataset_path
        self.sample_rate = sample_rate
        self.audio_length = audio_length
        self.use_fine_tuned_presets = use_fine_tuned_presets
        
        # ì•…ê¸°ì™€ í´ë” ë§¤í•‘
        self.instrument_mapping = {
            'electro guitar': 'Electro_Guitar',
            'acoustic guitar': 'Acoustic_Guitar', 
            'bass guitar': 'Bass_Guitar',
            'piano': 'Piano',
            'keyboard': 'Keyboard',
            'organ': 'Organ',
            'drum set': 'Drum_set',
            'violin': 'Violin',
            'flute': 'flute',
            'saxophone': 'Saxophone',
            'trumpet': 'Trumpet'
        }
        
        # Fine-tuned presets ë¡œë“œ (ì´ˆê¸° ê°€ì´ë“œìš©)
        self.fine_presets = []
        if use_fine_tuned_presets and fine_preset_path and os.path.exists(fine_preset_path):
            self.fine_presets = self._load_fine_presets(fine_preset_path)
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   - Description ìˆ˜: {len(self.descriptions)}")
        print(f"   - Fine presets ìˆ˜: {len(self.fine_presets)}")
        print(f"   - ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_length}ì´ˆ")
        
    def _load_fine_presets(self, preset_path):
        """Fine-tuned presets íŒŒì¼ ë¡œë“œ (Python eval ë°©ì‹)"""
        presets = []
        try:
            with open(preset_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"ğŸ“– Fine preset íŒŒì¼ ì½ê¸° ì‹œì‘: {preset_path}")
            
            # Python dictionary í˜•íƒœë¡œ íŒŒì‹± - ì¤‘ê´„í˜¸ ë¸”ë¡ ì¶”ì¶œ
            blocks = []
            i = 0
            while i < len(content):
                if content[i] == '{':
                    # ì‹œì‘ì  ì°¾ìŒ
                    start = i
                    brace_count = 1
                    i += 1
                    
                    while i < len(content) and brace_count > 0:
                        if content[i] == '{':
                            brace_count += 1
                        elif content[i] == '}':
                            brace_count -= 1
                        i += 1
                    
                    if brace_count == 0:
                        block = content[start:i]
                        # "prompt"ê°€ í¬í•¨ëœ ë¸”ë¡ë§Œ ì„ íƒ
                        if '"prompt"' in block or "'prompt'" in block:
                            blocks.append(block)
                else:
                    i += 1
            
            # ê° ë¸”ë¡ì„ Python dictionaryë¡œ íŒŒì‹±
            for i, block in enumerate(blocks):
                try:
                    # ì£¼ì„ ì œê±° í•¨ìˆ˜ (ë¬¸ìì—´ ë‚´ë¶€ëŠ” ì œì™¸)
                    def remove_python_comments(text):
                        lines = text.split('\n')
                        cleaned_lines = []
                        for line in lines:
                            # # ì´í›„ ëª¨ë“  ë‚´ìš© ì œê±° (ë¬¸ìì—´ ë‚´ë¶€ê°€ ì•„ë‹Œ ê²½ìš°)
                            in_string = False
                            quote_char = None
                            comment_pos = -1
                            
                            for j, char in enumerate(line):
                                if char in ['"', "'"] and (j == 0 or line[j-1] != '\\'):
                                    if not in_string:
                                        in_string = True
                                        quote_char = char
                                    elif char == quote_char:
                                        in_string = False
                                        quote_char = None
                                elif char == '#' and not in_string:
                                    comment_pos = j
                                    break
                            
                            if comment_pos >= 0:
                                line = line[:comment_pos]
                            
                            line = line.rstrip()
                            if line:
                                cleaned_lines.append(line)
                        
                        return '\n'.join(cleaned_lines)
                    
                    # ì£¼ì„ ì œê±°
                    cleaned_block = remove_python_comments(block)
                    
                    # Pythonì˜ evalë¡œ dictionary íŒŒì‹± (ì•ˆì „í•˜ê²Œ)
                    preset = eval(cleaned_block)
                    
                    if isinstance(preset, dict) and 'prompt' in preset:
                        presets.append(preset)
                    
                except Exception as e:
                    if i < 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ë§Œ ì¶œë ¥
                        print(f"âŒ Failed parsing preset {i+1}: {str(e)[:100]}")
                        print(f"   Block preview: {block[:200]}...")
                    continue
                    
        except Exception as e:
            print(f"âŒ Failed loading fine presets: {e}")
            import traceback
            traceback.print_exc()
            
        print(f"ğŸ¯ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ preset ìˆ˜: {len(presets)}")
        
        return presets
    
    def _extract_keywords_from_description(self, description):
        """Descriptionì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ì˜¤ë””ì˜¤ íš¨ê³¼ ê´€ë ¨ í‚¤ì›Œë“œë“¤
        effect_keywords = [
            'reverb', 'echo', 'delay', 'room', 'hall', 'large', 'small', 'cave', 'cathedral',
            'distortion', 'overdrive', 'fuzz', 'saturation', 'warm', 'bright', 'dark',
            'chorus', 'flanger', 'phaser', 'tremolo', 'vibrato', 'modulation',
            'bass', 'treble', 'mid', 'low', 'high', 'frequency',
            'whisper', 'shout', 'loud', 'quiet', 'soft', 'harsh', 'smooth',
            'monster', 'robot', 'alien', 'fairy', 'giant', 'tiny', 'deep', 'thin',
            'male', 'female', 'child', 'old', 'young',
            'guitar', 'piano', 'drum', 'vocal', 'singing', 'speaking'
        ]
        
        # Descriptionì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë‹¨ì–´ ë¶„í• 
        words = description.lower().split()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        for word in words:
            # êµ¬ë‘ì  ì œê±°
            clean_word = word.strip('.,!?()[]{}";:')
            if clean_word in effect_keywords:
                keywords.append(clean_word)
            
            # ë¶€ë¶„ ë§¤ì¹­ë„ í™•ì¸
            for keyword in effect_keywords:
                if keyword in clean_word and len(keyword) > 3:
                    keywords.append(keyword)
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    def _extract_subject_from_description(self, description):
        """Descriptionì—ì„œ ì£¼ì²´(ì•…ê¸°/ì‚¬ëŒ) ì¶”ì¶œ"""
        description_lower = description.lower()
        
        # ì‚¬ëŒ ì£¼ì²´ í™•ì¸
        if any(word in description_lower for word in ['male', 'female', 'speaks', 'whispers', 'singing', 'says']):
            if 'male' in description_lower:
                return 'male', 'speech'
            elif 'female' in description_lower:
                return 'female', 'speech'
            else:
                return 'male', 'speech'  # ê¸°ë³¸ê°’
        
        # ì•…ê¸° ì£¼ì²´ í™•ì¸
        for instrument, folder in self.instrument_mapping.items():
            if instrument in description_lower:
                return instrument, 'instrumental'
        
        # ê¸°ë³¸ê°’: í”¼ì•„ë…¸
        return 'piano', 'instrumental'
    
    def _get_random_audio_file(self, subject, audio_type):
        """ì£¼ì²´ì— ë§ëŠ” ëœë¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ"""
        if audio_type == 'speech':
            folder_path = os.path.join(self.audio_dataset_path, 'speech', subject)
        else:  # instrumental
            folder_name = self.instrument_mapping.get(subject, 'Piano')
            folder_path = os.path.join(self.audio_dataset_path, 'instrumentals', folder_name)
        
        if not os.path.exists(folder_path):
            print(f"âš ï¸  í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {folder_path}")
            # ëŒ€ì²´ í´ë” ì‚¬ìš©
            folder_path = os.path.join(self.audio_dataset_path, 'instrumentals', 'Piano')
        
        # WAV, FLAC íŒŒì¼ë“¤ ì°¾ê¸° (ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í˜•ì‹ ì§€ì›)
        audio_files = []
        for ext in ['*.wav', '*.flac', '*.mp3', '*.m4a']:
            audio_files.extend(list(Path(folder_path).glob(ext)))
        
        if not audio_files:
            print(f"âš ï¸  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {folder_path}")
            return None
        
        return str(random.choice(audio_files))
    
    def _load_audio(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.audio_length)
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            target_length = int(self.sample_rate * self.audio_length)
            if len(audio) < target_length:
                # íŒ¨ë”©
                audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
            else:
                # ìë¥´ê¸°
                audio = audio[:target_length]
            
            return torch.FloatTensor(audio)
        
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ {audio_path}: {e}")
            # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°˜í™˜
            target_length = int(self.sample_rate * self.audio_length)
            return torch.zeros(target_length)
    
    def __len__(self):
        return len(self.descriptions)
    
    def __getitem__(self, idx):
        description = self.descriptions[idx]
        
        # ì£¼ì²´ ì¶”ì¶œ
        subject, audio_type = self._extract_subject_from_description(description)
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ ë° ë¡œë“œ
        audio_path = self._get_random_audio_file(subject, audio_type)
        if audio_path is None:
            # ë”ë¯¸ ì˜¤ë””ì˜¤
            target_length = int(self.sample_rate * self.audio_length)
            audio = torch.zeros(target_length)
        else:
            audio = self._load_audio(audio_path)
        
        # Fine preset ê°€ì´ë“œ (ìŠ¤ë§ˆíŠ¸ í• ë‹¹)
        guide_preset = None
        if self.use_fine_tuned_presets and self.fine_presets:
            # 1ë‹¨ê³„: Descriptionê³¼ ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” preset ì°¾ê¸°
            for preset in self.fine_presets:
                preset_prompt = preset.get('prompt', '').strip().lower()
                if preset_prompt and (preset_prompt in description.lower() or description.lower() in preset_prompt):
                    guide_preset = preset
                    break
            
            # 2ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ì„ íƒì  í• ë‹¹ (í™•ë¥  ê¸°ë°˜)
            if guide_preset is None and self.fine_presets:
                import random
                # ê°€ì´ë“œ í”„ë¦¬ì…‹ ì‚¬ìš© í™•ë¥  (30%ë§Œ ê°€ì´ë“œ ì‚¬ìš©, 70%ëŠ” ìˆœìˆ˜ description í•™ìŠµ)
                guide_usage_probability = 0.3
                
                if random.random() < guide_usage_probability:
                    # ìœ ì‚¬í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ ì‹œë„
                    description_keywords = self._extract_keywords_from_description(description.lower())
                    best_match_preset = None
                    best_score = 0
                    
                    for preset in self.fine_presets:
                        preset_prompt = preset.get('prompt', '').strip().lower()
                        preset_keywords = self._extract_keywords_from_description(preset_prompt)
                        
                        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                        common_keywords = set(description_keywords) & set(preset_keywords)
                        if common_keywords:
                            score = len(common_keywords) / max(len(description_keywords), len(preset_keywords))
                            if score > best_score:
                                best_score = score
                                best_match_preset = preset
                    
                    # ìµœì†Œ ì ìˆ˜ ì´ìƒì´ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ëœë¤
                    if best_match_preset and best_score > 0.1:
                        guide_preset = best_match_preset
                        if idx < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                            print(f"ğŸ¯ Item {idx}: í‚¤ì›Œë“œ ë§¤ì¹­ guide preset (score: {best_score:.2f})")
                            print(f"   - Description: {description[:50]}...")
                            print(f"   - Guide prompt: {best_match_preset.get('prompt', 'No prompt')[:50]}...")
                    else:
                        # ë‚®ì€ í™•ë¥ ë¡œ ëœë¤ í• ë‹¹ (10%)
                        if random.random() < 0.1:
                            guide_preset = random.choice(self.fine_presets)
                            if idx < 2:  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                                print(f"ğŸ¯ Item {idx}: ëœë¤ guide preset í• ë‹¹")
                # else: guide_presetì€ Noneìœ¼ë¡œ ìœ ì§€ (ìˆœìˆ˜ description í•™ìŠµ)
        
        return {
            'description': description,
            'audio': audio,
            'subject': subject,
            'audio_type': audio_type,
            'guide_preset': guide_preset if guide_preset is not None else {}  # None ëŒ€ì‹  ë¹ˆ dict ë°˜í™˜
        }
    



class TrainingManager:
    """í›ˆë ¨ ê´€ë¦¬ì - ë©€í‹°GPU ì§€ì›"""
    
    def __init__(self, args, rank=0, world_size=1):
        self.args = args
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')
        
        # ë¶„ì‚° í›ˆë ¨ ì´ˆê¸°í™”
        if world_size > 1:
            self.setup_distributed()
        
        # CLAP ë¡œë”© ì‹œ ì¶œë ¥ ì–µì œ
        import sys
        from contextlib import redirect_stdout, redirect_stderr
        import os
        
        if self.rank == 0:
            print("ğŸ”„ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... (CLAP ë¡œë”©)")
        
        # stdout/stderr ì„ì‹œ ì–µì œ (CLAP verbose ì¶œë ¥ ë°©ì§€)
        with open(os.devnull, 'w') as devnull:
            with redirect_stdout(devnull), redirect_stderr(devnull):
                # DynamicPipelineFactoryë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì´ˆê¸°í™”
                factory = DynamicPipelineFactory()
                
                # argsì—ì„œ text_encoder_type ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: sentence-transformer-large)
                text_encoder_type = getattr(args, 'text_encoder_type', 'sentence-transformer-large')
                
                # ë™ì  íŒŒì´í”„ë¼ì¸ ìƒì„± - ì°¨ì›ì€ ìë™ìœ¼ë¡œ ê²°ì •ë¨
                self.model = factory.create_pipeline(
                    encoder_preset=text_encoder_type,
                    use_clap=True,
                    backbone_type='residual',
                    decoder_type='parallel',
                    sample_rate=args.sample_rate,
                    use_differentiable_audio=True,
                    target_params=500000
                )
                
                # ì‹¤ì œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì› ì—…ë°ì´íŠ¸ (ë™ì ìœ¼ë¡œ ê²°ì •ëœ ê°’)
                if hasattr(self.model, 'text_encoder'):
                    actual_text_dim = self.model.text_encoder.get_embedding_dim()
                    args.text_embed_dim = actual_text_dim  # args ì—…ë°ì´íŠ¸
                    if self.rank == 0:
                        print(f"ğŸ”§ ì‹¤ì œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {actual_text_dim}")
                        print(f"   ì„ íƒëœ ì¸ì½”ë”: {text_encoder_type}")
                
                # ë°±ë³¸ ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì„± í™•ì¸
                if hasattr(self.model, 'backbone'):
                    if hasattr(self.model.backbone, 'text_dim'):
                        backbone_text_dim = self.model.backbone.text_dim
                        if self.rank == 0:
                            print(f"ğŸ”§ ë°±ë³¸ í…ìŠ¤íŠ¸ ì…ë ¥ ì°¨ì›: {backbone_text_dim}")
                    if hasattr(self.model.backbone, 'clap_dim'):
                        backbone_clap_dim = self.model.backbone.clap_dim
                        if self.rank == 0:
                            print(f"ğŸ”§ ë°±ë³¸ CLAP ì…ë ¥ ì°¨ì›: {backbone_clap_dim}")
        
        # ëª…ì‹œì ìœ¼ë¡œ deviceë¡œ ì´ë™ (verbose ì¶œë ¥ì„ ìœ„í•´)
        if self.rank == 0:
            print(f"ğŸš€ ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™ ì¤‘...")
        self.model = self.model.to(self.device)
        
        # Pipeline ë‚´ë¶€ì˜ CLAP ëª¨ë“ˆì— ì ‘ê·¼ (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)
        self.clap_module = self.model.clap_encoder if hasattr(self.model, 'clap_encoder') else None
        
        # ë©€í‹°GPUìš© ëª¨ë¸ ë˜í•‘
        if world_size > 1:
            self.model = DDP(self.model, device_ids=[rank], output_device=rank)
            # CLAPì€ ì´ë¯¸ model ë‚´ë¶€ì— ìˆìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ë˜í•‘í•˜ì§€ ì•ŠìŒ
        elif torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            # CLAPì€ ì´ë¯¸ model ë‚´ë¶€ì— ìˆìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ë˜í•‘í•˜ì§€ ì•ŠìŒ
            
        # Optimizer - ì˜¤ì§ model parametersë§Œ (CLAPì€ frozen)
        model_params = list(self.model.parameters())
        trainable_params = [p for p in model_params if p.requires_grad]
        
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        # Alternative: ReduceLROnPlateau for adaptive learning rate
        # self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        #     self.optimizer, mode='min', factor=0.5, patience=10, verbose=True
        # )
        
        # Loss function
        self.criterion = nn.CosineEmbeddingLoss()
        
        if self.rank == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì¶œë ¥
            model_params = sum(p.numel() for p in self.model.parameters())
            model_trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_trainable = model_trainable
            
            print(f"ğŸš€ í›ˆë ¨ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - Device: {self.device}")
            print(f"   - World size: {world_size}")
            print(f"   - Model parameters: {model_params:,} ({model_trainable:,} trainable)")
            print(f"   - Total trainable: {total_trainable:,}")
            print(f"   - CLAP ì‚¬ìš©: {'âœ… Enabled (ë‚´ì¥)' if self.clap_module else 'âŒ Disabled'}")
            
            # ë” ìƒì„¸í•œ ëª¨ë¸ êµ¬ì„± ë° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            print(f"\nğŸ“Š ëª¨ë¸ êµ¬ì„± ìƒì„¸:")
            
            # ê° êµ¬ì„±ìš”ì†Œë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ë° ë””ë°”ì´ìŠ¤ ìœ„ì¹˜ ê³„ì‚°
            if hasattr(self.model, 'text_encoder'):
                text_encoder_params = sum(p.numel() for p in self.model.text_encoder.parameters())
                # Text Encoderì˜ ì‹¤ì œ ë””ë°”ì´ìŠ¤ í™•ì¸
                te_devices = set()
                for name, param in self.model.text_encoder.named_parameters():
                    te_devices.add(str(param.device))
                    if len(te_devices) <= 3:  # ì²˜ìŒ ëª‡ ê°œë§Œ ìƒì„¸ ì¶œë ¥
                        print(f"      - {name[:30]}... : {param.device}")
                print(f"   ğŸ“ Text Encoder: {text_encoder_params:,} parameters")
                print(f"      Devices: {list(te_devices)}")
            
            if hasattr(self.model, 'clap_encoder') and self.model.clap_encoder:
                clap_params = sum(p.numel() for p in self.model.clap_encoder.parameters())
                clap_trainable = sum(p.numel() for p in self.model.clap_encoder.parameters() if p.requires_grad)
                
                # CLAP ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤ë“¤ í™•ì¸
                clap_devices = set()
                for name, param in self.model.clap_encoder.named_parameters():
                    clap_devices.add(str(param.device))
                    if len(clap_devices) <= 3:  # ì²˜ìŒ ëª‡ ê°œë§Œ ìƒì„¸ ì¶œë ¥
                        print(f"      - {name[:30]}... : {param.device}")
                
                print(f"   ğŸµ CLAP Encoder: {clap_params:,} parameters ({clap_trainable:,} trainable)")
                print(f"      Devices: {list(clap_devices)}")
                
                # CLAP ë‚´ë¶€ ëª¨ë¸ êµ¬ì¡° í™•ì¸
                if hasattr(self.model.clap_encoder, 'clap_model'):
                    inner_clap_devices = set()
                    for name, param in self.model.clap_encoder.clap_model.named_parameters():
                        inner_clap_devices.add(str(param.device))
                    print(f"      Inner CLAP Model Devices: {list(inner_clap_devices)}")
            
            if hasattr(self.model, 'backbone'):
                backbone_params = sum(p.numel() for p in self.model.backbone.parameters())
                backbone_devices = set()
                for name, param in self.model.backbone.named_parameters():
                    backbone_devices.add(str(param.device))
                print(f"   ğŸ§  Backbone: {backbone_params:,} parameters")
                print(f"      Devices: {list(backbone_devices)}")
            
            if hasattr(self.model, 'decoder'):
                decoder_params = sum(p.numel() for p in self.model.decoder.parameters())
                decoder_devices = set()
                for name, param in self.model.decoder.named_parameters():
                    decoder_devices.add(str(param.device))
                print(f"   ğŸ›ï¸ Decoder: {decoder_params:,} parameters")
                print(f"      Devices: {list(decoder_devices)}")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(self.device) / 1024**3  # GB
                reserved = torch.cuda.memory_reserved(self.device) / 1024**3   # GB
                max_memory = torch.cuda.get_device_properties(self.device).total_memory / 1024**3  # GB
                
                print(f"\nğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
                print(f"   - Allocated: {allocated:.2f} GB")
                print(f"   - Reserved: {reserved:.2f} GB") 
                print(f"   - Total GPU Memory: {max_memory:.2f} GB")
                print(f"   - Usage: {(allocated/max_memory)*100:.1f}% allocated, {(reserved/max_memory)*100:.1f}% reserved")
                
                # ëª¨ë¸ì´ ì‹¤ì œë¡œ GPUì— ìˆëŠ”ì§€ í™•ì¸
                model_on_gpu = next(self.model.parameters()).device
                print(f"   - Model Device: {model_on_gpu}")
            
            print("=" * 60)
    

    
    def setup_distributed(self):
        """ë¶„ì‚° í›ˆë ¨ ì„¤ì •"""
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        dist.init_process_group(
            backend='nccl',
            rank=self.rank,
            world_size=self.world_size
        )
    
    def load_datasets(self):
        """ë°ì´í„°ì…‹ ë¡œë“œ - ë¶„ì‚° í›ˆë ¨ ì§€ì›"""
        # Descriptions ë¡œë“œ
        descriptions = []
        
        if self.args.use_sampled_descriptions:
            # 500ê°œ ìƒ˜í”Œ íŒŒì¼ ì‚¬ìš©
            desc_file = os.path.join(self.args.data_path, 'descriptions', '500_sampled_descriptions.txt')
        else:
            # ì „ì²´ descriptions íŒŒì¼ ì‚¬ìš©
            desc_file = os.path.join(self.args.data_path, 'descriptions', 'descriptions.txt')
        print(f"ğŸ“‚ Description íŒŒì¼ ê²½ë¡œ: {desc_file}")
        if os.path.exists(desc_file):
            with open(desc_file, 'r', encoding='utf-8') as f:
                descriptions = [line.strip() for line in f if line.strip()]
        else:
            if self.rank == 0:
                print(f"âŒ Description íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {desc_file}")
            return None, None
        
        # ìµœëŒ€ description ìˆ˜ ì œí•œ
        if self.args.max_descriptions > 0 and len(descriptions) > self.args.max_descriptions:
            if self.rank == 0:
                print(f"ğŸ“Š Description ìˆ˜ ì œí•œ: {len(descriptions)} â†’ {self.args.max_descriptions}")
            random.seed(42)
            descriptions = random.sample(descriptions, self.args.max_descriptions)
        
        if self.rank == 0:
            print(f"ğŸ“š {len(descriptions)}ê°œ description ë¡œë“œë¨")
            if len(descriptions) > 100000:
                print(f"âš ï¸  ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹! í•œ ì—í¬í¬ì— ì•½ {len(descriptions):,}ê°œ description ì‚¬ìš©")
                print(f"   --max_descriptions ì˜µì…˜ìœ¼ë¡œ ì œí•œ ê¶Œì¥ (ì˜ˆ: --max_descriptions 50000)")
        
        # ë°ì´í„°ì…‹ ë¶„í•  (ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì¼í•œ ì‹œë“œ ì‚¬ìš©)
        random.seed(42)
        random.shuffle(descriptions)
        split_idx = int(len(descriptions) * 0.8)
        train_descriptions = descriptions[:split_idx]
        val_descriptions = descriptions[split_idx:]
        
        # Fine preset ê²½ë¡œ
        fine_preset_path = os.path.join(self.args.data_path, 'descriptions', 'fined_presets.txt')
        
        # Custom collate function for handling guide_presets
        def custom_collate_fn(batch):
            """Custom collate function to properly handle guide_presets"""
            descriptions = [item['description'] for item in batch]
            audios = torch.stack([item['audio'] for item in batch])
            subjects = [item['subject'] for item in batch]
            audio_types = [item['audio_type'] for item in batch]
            guide_presets = [item['guide_preset'] for item in batch]  # List of dicts or empty dicts
            
            return {
                'description': descriptions,
                'audio': audios,
                'subject': subjects,
                'audio_type': audio_types,
                'guide_preset': guide_presets  # Keep as list
            }
        
        # í›ˆë ¨ ë°ì´í„°ì…‹ (ê°€ì´ë“œ í”„ë¦¬ì…‹ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼)
        train_dataset = PresetDataset(
            descriptions=train_descriptions,
            audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
            use_fine_tuned_presets=self.args.use_guide_presets,  # argumentì— ë”°ë¼ ê²°ì •
            fine_preset_path=fine_preset_path if self.args.use_guide_presets else None,
            sample_rate=self.args.sample_rate,
            audio_length=self.args.audio_length
        )
        
        # ê²€ì¦ ë°ì´í„°ì…‹
        val_dataset = PresetDataset(
            descriptions=val_descriptions,
            audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
            use_fine_tuned_presets=False,
            sample_rate=self.args.sample_rate,
            audio_length=self.args.audio_length
        )
        
        # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì„¤ì •
        train_sampler = None
        val_sampler = None
        if self.world_size > 1:
            train_sampler = DistributedSampler(
                train_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
            val_sampler = DistributedSampler(
                val_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False
            )
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.args.batch_size,
            shuffle=(train_sampler is None),
            sampler=train_sampler,
            num_workers=self.args.num_workers,
            pin_memory=True,
            collate_fn=custom_collate_fn  # Use custom collate function
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            sampler=val_sampler,
            num_workers=self.args.num_workers,
            pin_memory=True,
            collate_fn=custom_collate_fn  # Use custom collate function
        )
        
        return train_loader, val_loader
    
    def compute_clap_loss_batch_wrapper(self, processed_audio_batch, target_descriptions):
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ - ê¸°ì¡´ compute_clap_loss ì‚¬ìš©"""
        try:
            # ê¸°ì¡´ compute_clap_lossëŠ” ì´ë¯¸ ë°°ì¹˜ë¥¼ ì§€ì›í•¨
            return self.compute_clap_loss(processed_audio_batch, target_descriptions)
        except Exception as e:
            print(f"âŒ CLAP batch wrapper ì‹¤íŒ¨: {e}")
            # fallback
            dummy_param = next(self.model.parameters())
            return torch.mean(dummy_param * 0.0) + 0.1

    def compute_clap_loss(self, processed_audio, target_description):
        """CLAP ê¸°ë°˜ ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ loss ê³„ì‚° (gradient ìœ ì§€)"""
        try:
            # Pipeline ë‚´ë¶€ì˜ CLAP ëª¨ë“ˆ ì ‘ê·¼ (DDP ê³ ë ¤)
            if hasattr(self.model, 'module'):
                # DDP wrapped model
                clap_module = getattr(self.model.module, 'clap_encoder', None)
            else:
                # Normal model
                clap_module = getattr(self.model, 'clap_encoder', None)
            
            if clap_module is None:
                print("âš ï¸  CLAP ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, fallback loss ì‚¬ìš©")
                return torch.tensor(1.0, device=self.device, requires_grad=True)
            
            # ë‹¨ì¼ description ì²˜ë¦¬
            if isinstance(target_description, list) and len(target_description) == 1:
                target_description = target_description[0]
            
            # Processed audioê°€ listì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì„ íƒí•˜ê±°ë‚˜ í…ì„œë¡œ ë³€í™˜
            if isinstance(processed_audio, list):
                if len(processed_audio) > 0:
                    processed_audio = processed_audio[0]  # ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                else:
                    print("âš ï¸  Processed audio listê°€ ë¹„ì–´ìˆìŒ")
                    return torch.tensor(1.0, device=self.device, requires_grad=True)
            
            # í…ì„œê°€ ì•„ë‹Œ ê²½ìš° í…ì„œë¡œ ë³€í™˜
            if not isinstance(processed_audio, torch.Tensor):
                print(f"âš ï¸  Processed audioê°€ í…ì„œê°€ ì•„ë‹˜: {type(processed_audio)}")
                return torch.tensor(1.0, device=self.device, requires_grad=True)
            
            # Processed audio ì°¨ì› ì²˜ë¦¬
            if processed_audio.dim() == 3:  # [batch, channels, samples]
                processed_audio = processed_audio.squeeze(1)  # [batch, samples]
            elif processed_audio.dim() == 1:  # [samples]
                processed_audio = processed_audio.unsqueeze(0)  # [1, samples]
            
            # CLAP loss ê³„ì‚° (ë‹¨ìˆœíˆ embedding ë¹„êµ)
            clap_loss = clap_module.compute_clap_loss(processed_audio, target_description)
            
            # CLAP lossê°€ gradientë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
            if not clap_loss.requires_grad:
                print(f"âš ï¸ CLAP lossê°€ gradientë¥¼ ê°€ì§€ì§€ ì•ŠìŒ, ë”ë¯¸ loss ìƒì„±")
                # gradientê°€ ìˆëŠ” ë”ë¯¸ loss ìƒì„± (ëª¨ë¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                dummy_param = next(self.model.parameters())
                clap_loss = torch.mean(dummy_param * 0.0) + 0.1  # ëª¨ë¸ê³¼ ì—°ê²°ëœ ë”ë¯¸ loss
            
            return clap_loss
            
        except Exception as e:
            print(f"âŒ CLAP loss ê³„ì‚° ì‹¤íŒ¨: {e}")
            # gradientê°€ ìˆëŠ” ë”ë¯¸ loss ìƒì„±
            try:
                dummy_param = next(self.model.parameters())
                fallback_loss = torch.mean(dummy_param * 0.0) + 0.1
                return fallback_loss
            except:
                return torch.tensor(1.0, device=self.device, requires_grad=True)
    
    def train_with_guide_preset(self, description, audio, guide_preset):
        """Guide presetì„ ì‚¬ìš©í•œ ê°œë³„ item í›ˆë ¨"""
        try:
            # Guide presetì´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if not guide_preset or not isinstance(guide_preset, dict):
                # gradientê°€ ìˆëŠ” ë”ë¯¸ loss ìƒì„±
                dummy_param = next(self.model.parameters())
                return torch.mean(dummy_param * 0.0) + 0.01
            
            # ëª¨ë¸ forward passë¡œ ì˜ˆì¸¡ëœ íŒŒë¼ë¯¸í„° ì–»ê¸°
            self.model.train()
            outputs = self.model(
                texts=[description],
                audio=audio,
                use_real_audio=False
            )
            
            if 'preset_params' not in outputs:
                # Preset paramsê°€ ì—†ìœ¼ë©´ CLAP lossë¡œ ëŒ€ì²´
                processed_audio = outputs.get('processed_audio', audio)
                return self.compute_clap_loss(processed_audio, description)
            
            # ì˜ˆì¸¡ëœ íŒŒë¼ë¯¸í„°ì™€ guide preset ê°„ì˜ MSE loss
            predicted_params = outputs['preset_params']
            guide_loss = self.compute_guide_loss(predicted_params, guide_preset)
            
            # Guide presetìœ¼ë¡œ ì‹¤ì œ ì˜¤ë””ì˜¤ ì²˜ë¦¬í•´ì„œ CLAP lossë„ ì¶”ê°€
            try:
                # Guide preset íŒŒë¼ë¯¸í„°ë¡œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ë§Œì•½ ê°€ëŠ¥í•˜ë‹¤ë©´)
                if 'processed_audio' in outputs:
                    processed_with_guide = outputs['processed_audio']
                    clap_loss = self.compute_clap_loss(processed_with_guide, description)
                    
                    # Guide loss (íŒŒë¼ë¯¸í„° ë§¤ì¹­) + CLAP loss (ê²°ê³¼ í’ˆì§ˆ)
                    total_loss = self.args.guide_weight * guide_loss + (1 - self.args.guide_weight) * clap_loss
                    return total_loss
                else:
                    return guide_loss
                    
            except Exception:
                # Guide lossë§Œ ì‚¬ìš©
                return guide_loss
            
        except Exception as e:
            print(f"âŒ Guide preset í›ˆë ¨ ì‹¤íŒ¨: {e}")
            # gradientê°€ ìˆëŠ” ë”ë¯¸ loss ìƒì„±
            try:
                dummy_param = next(self.model.parameters())
                fallback_loss = torch.mean(dummy_param * 0.0) + 0.1
                return fallback_loss
            except:
                return torch.tensor(0.1, device=self.device, requires_grad=True)
    
    def train_epoch(self, train_loader, epoch):
        """í•œ ì—í¬í¬ í›ˆë ¨ - Guide vs Description ì—í¬í¬ ë¶„ë¦¬"""
        self.model.train()
        total_loss = 0.0
        
        # í›ˆë ¨ ëª¨ë“œ ê²°ì •
        is_guide_epoch = self.args.use_guide_presets and epoch < self.args.guide_epochs
        if self.args.use_guide_presets:
            training_mode = "Guide Presets" if is_guide_epoch else "Text Descriptions"
        else:
            training_mode = "Text Descriptions"
        
        # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì—í¬í¬ ì„¤ì •
        if hasattr(train_loader.sampler, 'set_epoch'):
            train_loader.sampler.set_epoch(epoch)
        
        if self.rank == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ progress bar í‘œì‹œ
            desc = f"E{epoch+1}/{self.args.num_epochs} ({training_mode[:4]})"
            pbar = tqdm(train_loader, desc=desc, leave=False)  # leave=Falseë¡œ progress bar ì§€ìš°ê¸°
        else:
            pbar = train_loader
        
        for batch_idx, batch in enumerate(pbar):
            try:
                descriptions = batch['description']
                audios = batch['audio'].to(self.device, non_blocking=True)
                guide_presets = batch['guide_preset']
                
                # Audio tensor ì°¨ì› í™•ì¸ ë° ìˆ˜ì •
                if audios.dim() == 2:  # [batch_size, samples]
                    audios = audios.unsqueeze(1)  # [batch_size, 1, samples] -> mono ì±„ë„ ì¶”ê°€
                
                batch_loss = 0.0
                valid_items = 0
                
                if is_guide_epoch:
                    # Guide Preset ì—í¬í¬: guideê°€ ìˆëŠ” í•­ëª©ê³¼ ì—†ëŠ” í•­ëª©ì„ ê· í˜•ìˆê²Œ ì²˜ë¦¬
                    guide_descriptions = []
                    guide_audios = []
                    valid_guide_presets = []
                    
                    # Non-guide descriptions (ìˆœìˆ˜ description í•™ìŠµìš©)
                    non_guide_descriptions = []
                    non_guide_audios = []
                    
                    # Guide preset ì—¬ë¶€ì— ë”°ë¼ ë¶„ë¥˜
                    for i in range(len(descriptions)):
                        guide_preset = guide_presets[i] if isinstance(guide_presets, list) and i < len(guide_presets) else None
                        has_guide = isinstance(guide_presets, list) and i < len(guide_presets) and guide_preset is not None and bool(guide_preset)
                        
                        if has_guide:
                            guide_descriptions.append(descriptions[i])
                            guide_audios.append(audios[i])
                            valid_guide_presets.append(guide_preset)
                        else:
                            non_guide_descriptions.append(descriptions[i])
                            non_guide_audios.append(audios[i])
                    
                    # 1) Guide preset í•­ëª©ë“¤ ì²˜ë¦¬ (Guide Loss)
                    guide_loss_total = 0.0
                    guide_items = 0
                    
                    if guide_descriptions:
                        try:
                            # ì˜¤ë””ì˜¤ í…ì„œë“¤ì„ ì•ˆì „í•˜ê²Œ ìŠ¤íƒ
                            guide_audios_tensors = []
                            for audio_tensor in guide_audios:
                                if isinstance(audio_tensor, torch.Tensor):
                                    guide_audios_tensors.append(audio_tensor)
                                else:
                                    print(f"âš ï¸  ì˜¤ë””ì˜¤ê°€ í…ì„œê°€ ì•„ë‹˜: {type(audio_tensor)}")
                                    continue
                            
                            if guide_audios_tensors:
                                guide_audios_batch = torch.stack(guide_audios_tensors)  # [valid_items, channels, samples]
                                
                                # ëª¨ë¸ì— ë°°ì¹˜ë¡œ ì „ë‹¬
                                self.model.train()
                                outputs = self.model(
                                    texts=guide_descriptions,
                                    audio=guide_audios_batch,
                                    use_real_audio=False
                                )
                                
                                # Loss ëˆ„ì ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
                                individual_losses = []
                                
                                # ê° í•­ëª©ë³„ë¡œ guide loss ê³„ì‚°
                                for i in range(len(guide_descriptions)):
                                    if 'preset_params' in outputs:
                                        # ì˜ˆì¸¡ëœ íŒŒë¼ë¯¸í„°ì™€ guide preset ê°„ì˜ loss
                                        predicted_params = outputs['preset_params']
                                        if isinstance(predicted_params, list):
                                            predicted_params = predicted_params[i] if i < len(predicted_params) else predicted_params[0]
                                        elif isinstance(predicted_params, torch.Tensor) and predicted_params.dim() > 1:
                                            predicted_params = predicted_params[i:i+1]
                                        
                                        guide_loss = self.compute_guide_loss(predicted_params, valid_guide_presets[i])
                                        individual_losses.append(guide_loss)
                                    else:
                                        # Preset paramsê°€ ì—†ìœ¼ë©´ ë°°ì¹˜ CLAP loss ì‚¬ìš©
                                        processed_audio = outputs.get('processed_audio', guide_audios_batch)
                                        
                                        # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì˜¤ë””ì˜¤ì™€ descriptionë§Œ ì¶”ì¶œ
                                        if isinstance(processed_audio, torch.Tensor):
                                            processed_audio_i = processed_audio[i:i+1]
                                        else:
                                            processed_audio_i = guide_audios_batch[i:i+1]
                                        
                                        clap_loss = self.compute_clap_loss(processed_audio_i, [guide_descriptions[i]])
                                        individual_losses.append(clap_loss)
                                    
                                    guide_items += 1
                                
                                # Guide lossë“¤ì„ ì•ˆì „í•˜ê²Œ í•©ì‚°
                                if individual_losses:
                                    guide_loss_total = sum(individual_losses)
                                
                        except Exception as e:
                            if self.rank == 0:
                                print(f"âš ï¸ Guide batch ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
                                fallback_losses = []
                                for i in range(len(guide_descriptions)):
                                    try:
                                        audio_tensor = guide_audios[i]
                                        if not isinstance(audio_tensor, torch.Tensor):
                                            continue
                                        
                                        if audio_tensor.dim() == 2:
                                            audio_tensor = audio_tensor.unsqueeze(0)
                                        elif audio_tensor.dim() == 1:
                                            audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
                                        
                                        item_loss = self.train_with_guide_preset(
                                            guide_descriptions[i], audio_tensor, valid_guide_presets[i]
                                        )
                                        fallback_losses.append(item_loss)
                                        guide_items += 1
                                    except Exception as e2:
                                        continue
                                
                                if fallback_losses:
                                    guide_loss_total = sum(fallback_losses)
                    
                    # 2) Non-guide preset í•­ëª©ë“¤ ì²˜ë¦¬ (CLAP Loss) - Guide epochì—ì„œë„ ì¼ë¶€ ì²˜ë¦¬
                    clap_loss_total = 0.0
                    clap_items = 0
                    
                    # Guide epochì—ì„œë„ 30% ì •ë„ëŠ” ìˆœìˆ˜ description í•™ìŠµ
                    if non_guide_descriptions:
                        import random
                        sample_size = min(len(non_guide_descriptions), max(1, len(guide_descriptions) // 2))
                        sampled_indices = random.sample(range(len(non_guide_descriptions)), sample_size)
                        
                        sampled_descriptions = [non_guide_descriptions[i] for i in sampled_indices]
                        sampled_audios = [non_guide_audios[i] for i in sampled_indices]
                        
                        try:
                            sampled_audios_tensors = []
                            for audio_tensor in sampled_audios:
                                if isinstance(audio_tensor, torch.Tensor):
                                    sampled_audios_tensors.append(audio_tensor)
                            
                            if sampled_audios_tensors:
                                sampled_audios_batch = torch.stack(sampled_audios_tensors)
                                
                                self.model.train()
                                outputs = self.model(
                                    texts=sampled_descriptions,
                                    audio=sampled_audios_batch,
                                    use_real_audio=False
                                )
                                
                                individual_clap_losses = []
                                for i in range(len(sampled_descriptions)):
                                    if 'processed_audio' in outputs:
                                        processed_audio_i = outputs['processed_audio'][i:i+1] if isinstance(outputs['processed_audio'], torch.Tensor) else [outputs['processed_audio'][i]]
                                    else:
                                        processed_audio_i = sampled_audios_batch[i:i+1]
                                    
                                    # ë‹¨ì¼ ì•„ì´í…œì„ ë°°ì¹˜ í˜•íƒœë¡œ ì²˜ë¦¬
                                    if isinstance(processed_audio_i, list):
                                        processed_audio_i = torch.stack(processed_audio_i) if len(processed_audio_i) > 0 else sampled_audios_batch[i:i+1]
                                    
                                    clap_loss = self.compute_clap_loss(processed_audio_i, [sampled_descriptions[i]])
                                    individual_clap_losses.append(clap_loss)
                                    clap_items += 1
                                
                                if individual_clap_losses:
                                    clap_loss_total = sum(individual_clap_losses)
                        
                        except Exception as e:
                            if self.rank == 0:
                                print(f"âš ï¸ Non-guide batch ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # 3) ìµœì¢… ë°°ì¹˜ loss ê³„ì‚° (Guide + CLAP ì¡°í•©)
                    total_items = guide_items + clap_items
                    if total_items > 0:
                        if guide_loss_total != 0.0 and clap_loss_total != 0.0:
                            # Guide lossì™€ CLAP lossë¥¼ ê°€ì¤‘ í‰ê· 
                            guide_weight = self.args.guide_weight
                            batch_loss = (guide_weight * guide_loss_total / max(1, guide_items) + 
                                         (1 - guide_weight) * clap_loss_total / max(1, clap_items))
                            valid_items = total_items
                        elif guide_loss_total != 0.0:
                            batch_loss = guide_loss_total / max(1, guide_items)
                            valid_items = guide_items
                        elif clap_loss_total != 0.0:
                            batch_loss = clap_loss_total / max(1, clap_items)
                            valid_items = clap_items
                        else:
                            # Fallback loss
                            dummy_param = next(self.model.parameters())
                            batch_loss = torch.mean(dummy_param * 0.0) + 0.1
                            valid_items = 1
                    else:
                        # Fallback loss
                        dummy_param = next(self.model.parameters())
                        batch_loss = torch.mean(dummy_param * 0.0) + 0.1
                        valid_items = 1
                    
                    if self.rank == 0 and batch_idx == 0:
                        print(f"ğŸ“Š Guide epoch ë°°ì¹˜ ê²°ê³¼: Guide={guide_items}, CLAP={clap_items}, Total={total_items}/{len(descriptions)}")
                else:
                    # Description ì—í¬í¬: ëª¨ë“  í•­ëª©ì„ descriptionìœ¼ë¡œ ì²˜ë¦¬
                    try:
                        # ì „ì²´ ë°°ì¹˜ë¥¼ í•œë²ˆì— ì²˜ë¦¬
                        self.model.train()
                        outputs = self.model(
                            texts=descriptions,
                            audio=audios,
                            use_real_audio=False
                        )
                        
                        # ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆì— CLAP loss ê³„ì‚°
                        if 'processed_audio' in outputs:
                            processed_audio_batch = outputs['processed_audio']
                        else:
                            processed_audio_batch = audios
                        
                        # CLAP lossë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³„ì‚° (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
                        batch_loss = self.compute_clap_loss_batch_wrapper(processed_audio_batch, descriptions)
                        valid_items = len(descriptions)
                    
                    except Exception as e:
                        print(f"âŒ Description batch ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        # gradientê°€ ìˆëŠ” ë”ë¯¸ loss
                        dummy_param = next(self.model.parameters())
                        batch_loss = torch.mean(dummy_param * 0.0) + 0.1
                        valid_items = 1  # ë”ë¯¸ í•­ëª© 1ê°œë¡œ ì„¤ì •
                
                # Valid itemsê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if valid_items == 0:
                    if self.rank == 0:
                        print(f"âš ï¸ ë°°ì¹˜ {batch_idx}: ì²˜ë¦¬í•  í•­ëª© ì—†ìŒ")
                    continue
                
                # ë°°ì¹˜ ì²˜ë¦¬ì—ì„œëŠ” ì´ë¯¸ í‰ê·  lossê°€ ê³„ì‚°ë¨
                # batch_loss = batch_loss / valid_items  # ì œê±°
                
                # batch_lossê°€ gradientë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
                if not batch_loss.requires_grad:
                    if self.rank == 0:
                        print(f"âš ï¸ ë°°ì¹˜ {batch_idx}: Lossê°€ gradientë¥¼ ê°€ì§€ì§€ ì•ŠìŒ, ê±´ë„ˆë›°ê¸°")
                    continue
                
                # Backward pass
                self.optimizer.zero_grad()
                batch_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += batch_loss.item()
                
                # Progress bar ì—…ë°ì´íŠ¸ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
                if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                    # í˜„ì¬ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°
                    current_lr = self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.args.learning_rate
                    
                    pbar.set_postfix({
                        'Loss': f'{batch_loss.item():.4f}',
                        'AvgLoss': f'{total_loss/(batch_idx+1):.4f}',
                        'Items': f'{valid_items}/{len(descriptions)}',
                        'LR': f'{current_lr:.1e}',
                        'Mode': training_mode[:5]  # "Guide" or "Text "
                    })
                    
                    # wandb logging (ë°°ì¹˜ë³„) - 50ë²ˆë§ˆë‹¤ë§Œ ë¡œê¹…
                    if self.args.use_wandb and batch_idx % 50 == 0:
                        # ë°°ì¹˜ë³„ ë¡œê¹…ì€ ê¸€ë¡œë²Œ stepìœ¼ë¡œ ê³„ì‚°
                        global_step = epoch * len(train_loader) + batch_idx
                        wandb.log({
                            'batch_loss': batch_loss.item(),
                            'batch_idx': batch_idx,
                            'epoch': epoch + 1,
                            'training_mode': training_mode,
                            'learning_rate': self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.args.learning_rate
                        }, step=global_step)
                    
            except Exception as e:
                if self.rank == 0:
                    print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                continue
        
        # ë¶„ì‚° í›ˆë ¨ì—ì„œ loss í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            avg_loss = torch.tensor(total_loss / len(train_loader)).to(self.device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss / self.world_size
            final_loss = avg_loss.item()
        else:
            final_loss = total_loss / max(1, len(train_loader))
        
        # wandb logging (ì—í¬í¬ë³„)
        if self.rank == 0 and self.args.use_wandb:
            wandb.log({
                'train_loss_epoch': final_loss,
                'training_mode': training_mode
            }, step=epoch)
        
        return final_loss
    
    def compute_guide_loss(self, generated_preset, guide_preset):
        """Guide presetê³¼ì˜ ì°¨ì´ë¥¼ ì´ìš©í•œ ë³´ì¡° loss"""
        # ê°„ë‹¨í•œ MSE lossë¡œ êµ¬í˜„
        try:
            # Presetì„ tensorë¡œ ë³€í™˜ (êµ¬í˜„ í•„ìš”)
            guide_tensor = self.preset_to_tensor(guide_preset)
            
            if guide_tensor is not None and isinstance(generated_preset, torch.Tensor):
                guide_tensor = guide_tensor.to(self.device)
                # ì°¨ì› ë§ì¶”ê¸°
                if generated_preset.dim() != guide_tensor.dim():
                    if generated_preset.dim() == 2 and guide_tensor.dim() == 1:
                        guide_tensor = guide_tensor.unsqueeze(0)
                    elif generated_preset.dim() == 1 and guide_tensor.dim() == 2:
                        generated_preset = generated_preset.unsqueeze(0)
                
                # í¬ê¸° ë§ì¶”ê¸° (ë” ì‘ì€ ìª½ì— ë§ì¶¤)
                min_size = min(generated_preset.shape[-1], guide_tensor.shape[-1])
                generated_preset_trimmed = generated_preset[..., :min_size]
                guide_tensor_trimmed = guide_tensor[..., :min_size]
                
                return nn.MSELoss()(generated_preset_trimmed, guide_tensor_trimmed)
            else:
                # fallback: gradientê°€ ìˆëŠ” ë”ë¯¸ loss
                return torch.tensor(0.1, device=self.device, requires_grad=True)
        except Exception as e:
            print(f"âš ï¸ Guide loss ê³„ì‚° ì‹¤íŒ¨: {e}")
            return torch.tensor(0.1, device=self.device, requires_grad=True)
    
    def preset_to_tensor(self, preset_dict):
        """Preset dictionaryë¥¼ tensorë¡œ ë³€í™˜"""
        # ì´ ë¶€ë¶„ì€ ì‹¤ì œ preset êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„ í•„ìš”
        # ì˜ˆì‹œ êµ¬í˜„
        try:
            values = []
            
            # Equalizer parameters
            if 'Equalizer' in preset_dict:
                for eq in preset_dict['Equalizer']:
                    values.extend([eq.get('frequency', 0), eq.get('gain', 0), eq.get('q', 0)])
            
            # Reverb parameters  
            if 'Reverb' in preset_dict:
                reverb = preset_dict['Reverb']
                values.extend([
                    reverb.get('room_size', 0),
                    reverb.get('pre_delay', 0),
                    reverb.get('diffusion', 0),
                    reverb.get('damping', 0),
                    reverb.get('wet_gain', 0)
                ])
            
            # Distortion parameters
            if 'Distortion' in preset_dict:
                dist = preset_dict['Distortion']
                values.extend([
                    dist.get('gain', 0),
                    dist.get('color', 0)
                ])
            
            # Pitch parameters
            if 'Pitch' in preset_dict:
                pitch = preset_dict['Pitch']
                values.extend([pitch.get('scale', 0)])
            
            return torch.FloatTensor(values) if values else None
            
        except Exception as e:
            return None
    
    def validate(self, val_loader):
        """ê²€ì¦ - ë¶„ì‚° í›ˆë ¨ ì§€ì›"""
        self.model.eval()
        total_loss = 0.0
        
        pbar = val_loader
        if self.rank == 0:
            pbar = tqdm(val_loader, desc="Validation", leave=False)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(pbar):
                descriptions = batch['description']
                audios = batch['audio'].to(self.device, non_blocking=True)
                
                # Forward pass through pipeline (eval modeë¡œ ë¹ ë¥´ê²Œ)
                self.model.eval()  # ê²€ì¦ì—ì„œëŠ” eval ëª¨ë“œ ì‚¬ìš©
                outputs = self.model(
                    texts=descriptions,
                    audio=audios,
                    use_real_audio=False
                )
                
                # ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆì— CLAP loss ê³„ì‚°
                if 'processed_audio' in outputs:
                    processed_audio_batch = outputs['processed_audio']
                else:
                    processed_audio_batch = audios
                
                # CLAP lossë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³„ì‚° (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
                batch_loss = self.compute_clap_loss(processed_audio_batch, descriptions)
                total_loss += batch_loss.item()
                
                # Progress bar ì—…ë°ì´íŠ¸ (validation)
                if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                    pbar.set_postfix({
                        'ValLoss': f'{batch_loss.item():.4f}',
                        'AvgValLoss': f'{total_loss/(batch_idx+1):.4f}',
                        'Samples': len(descriptions)
                    })
        
        # ë¶„ì‚° í›ˆë ¨ì—ì„œ validation loss í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            avg_loss = torch.tensor(total_loss / len(val_loader)).to(self.device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss / self.world_size
            final_val_loss = avg_loss.item()
        else:
            final_val_loss = total_loss / len(val_loader)
        
        return final_val_loss
    
    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ"""
        if self.rank != 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì•„ë‹ˆë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
            return
            
        # DDP ëª¨ë¸ì˜ ê²½ìš° module ì ‘ê·¼ í•„ìš”
        model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'args': self.args
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸
        checkpoint_path = os.path.join(self.args.save_dir, f'checkpoint_epoch_{epoch}.pt')
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸
        if is_best:
            best_path = os.path.join(self.args.save_dir, 'best_model.pt')
            torch.save(checkpoint, best_path)
    
    def cleanup_distributed(self):
        """ë¶„ì‚° í›ˆë ¨ ì •ë¦¬"""
        if self.world_size > 1:
            dist.destroy_process_group()
    
    def train(self):
        """ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤"""
        if self.rank == 0:
            print("ğŸ¯ í›ˆë ¨ ì‹œì‘")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        train_loader, val_loader = self.load_datasets()
        if train_loader is None:
            if self.rank == 0:
                print("âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
        if self.rank == 0:
            os.makedirs(self.args.save_dir, exist_ok=True)
        
        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™”
        if self.world_size > 1:
            dist.barrier()
        
        # í›ˆë ¨ ë£¨í”„
        best_val_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            # í›ˆë ¨
            train_loss = self.train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            val_loss = self.validate(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # ë¡œê¹… (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
            if self.rank == 0:
                training_mode = "Text Descriptions"
                if self.args.use_guide_presets:
                    training_mode = "Guide Presets" if epoch < self.args.guide_epochs else "Text Descriptions"
                
                current_lr = self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.args.learning_rate
                
                # í•™ìŠµ ì§„í–‰ ìƒí™© ë” ìì„¸íˆ ì¶œë ¥
                improvement = ""
                if epoch > 0:
                    if hasattr(self, 'prev_train_loss'):
                        train_diff = train_loss - self.prev_train_loss
                        val_diff = val_loss - self.prev_val_loss
                        improvement = f" (Î”Train: {train_diff:+.6f}, Î”Val: {val_diff:+.6f})"
                
                # í˜„ì¬ ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
                print(f"Epoch {epoch+1}/{self.args.num_epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}, LR={current_lr:.1e} ({training_mode}){improvement}")
                
                # ì´ì „ loss ì €ì¥
                self.prev_train_loss = train_loss
                self.prev_val_loss = val_loss
                
                # í•™ìŠµ ì •ì²´ ê°ì§€
                if epoch > 10:  # 10 ì—í¬í¬ í›„ë¶€í„° í™•ì¸
                    if abs(train_diff) < 1e-6 and abs(val_diff) < 1e-6:
                        print("âš ï¸  í•™ìŠµì´ ì •ì²´ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. Learning rate ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
                
                if self.args.use_wandb:
                    # wandb logging - ê¸€ë¡œë²Œ step ì‚¬ìš© (ë°°ì¹˜ ë‹¨ìœ„)
                    global_step = epoch * len(train_loader) + len(train_loader)  # ì—í¬í¬ ëì˜ step
                    wandb.log({
                        'epoch': epoch + 1,
                        'train_loss_epoch': train_loss,
                        'val_loss_epoch': val_loss,
                        'learning_rate_epoch': self.scheduler.get_last_lr()[0],
                        'training_mode_epoch': training_mode,
                        'is_guide_epoch': self.args.use_guide_presets and epoch < self.args.guide_epochs,
                        'use_guide_presets': self.args.use_guide_presets,
                        'guide_epochs': self.args.guide_epochs if self.args.use_guide_presets else 0
                    }, step=global_step)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
            
            if (epoch + 1) % self.args.save_every == 0 or is_best:
                self.save_checkpoint(epoch, val_loss, is_best)
        
        if self.rank == 0:
            print(f"âœ… í›ˆë ¨ ì™„ë£Œ! Best validation loss: {best_val_loss:.4f}")
            
            # Wandb ì¢…ë£Œ
            if self.args.use_wandb:
                try:
                    wandb.finish()
                except:
                    pass


def train_worker(rank, world_size, args):
    """ë©€í‹°GPU í›ˆë ¨ ì›Œì»¤ í•¨ìˆ˜"""
    try:
        # í›ˆë ¨ ì‹œì‘
        trainer = TrainingManager(args, rank, world_size)
        trainer.train()
    finally:
        # ì •ë¦¬
        if world_size > 1:
            trainer.cleanup_distributed()


def main():
    parser = argparse.ArgumentParser(description='Audio Effect Preset Generation Training')
    
    # ë°ì´í„° ê´€ë ¨
    parser.add_argument('--data_path', type=str, default='/workspace/AudioManipulator',
                       help='ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ')
    parser.add_argument('--sample_rate', type=int, default=44100,
                       help='ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë ˆì´íŠ¸')
    parser.add_argument('--audio_length', type=float, default=5.0,
                       help='ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)')
    parser.add_argument('--max_descriptions', type=int, default=50000,
                       help='ì‚¬ìš©í•  ìµœëŒ€ description ìˆ˜ (0=ëª¨ë‘ ì‚¬ìš©)')
    parser.add_argument('--use_sampled_descriptions', action='store_true', default=False,
                       help='500ê°œ ìƒ˜í”Œ description íŒŒì¼ ì‚¬ìš©')
    
    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument('--text_encoder_type', type=str, default='sentence-transformer-large',
                       choices=['sentence-transformer-mini', 'sentence-transformer-large', 'e5-large', 'clap'],
                       help='ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì¸ì½”ë” íƒ€ì…')
    parser.add_argument('--text_embed_dim', type=int, default=512,
                       help='í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--audio_embed_dim', type=int, default=1024,
                       help='ì˜¤ë””ì˜¤ ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--hidden_dim', type=int, default=256,
                       help='íˆë“  ë ˆì´ì–´ ì°¨ì›')
    
    # í›ˆë ¨ ê´€ë ¨
    parser.add_argument('--batch_size', type=int, default=32,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_epochs', type=int, default=100,
                       help='ì „ì²´ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=1e-2,
                       help='í•™ìŠµë¥ ')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='Weight decay')
    
    # ê°€ì´ë“œ í”„ë¦¬ì…‹ ê´€ë ¨
    parser.add_argument('--use_guide_presets', action='store_true', default=False,
                       help='Fine-tuned guide preset ì‚¬ìš© ì—¬ë¶€')
    parser.add_argument('--guide_epochs', type=int, default=20,
                       help='Guide presetì„ ì‚¬ìš©í•  ì—í¬í¬ ìˆ˜ (use_guide_presetsê°€ Trueì¼ ë•Œë§Œ)')
    parser.add_argument('--guide_weight', type=float, default=0.5,
                       help='Guide lossì˜ ê°€ì¤‘ì¹˜ (use_guide_presetsê°€ Trueì¼ ë•Œë§Œ)')
    
    # ì‹œìŠ¤í…œ ê´€ë ¨
    parser.add_argument('--num_workers', type=int, default=4,
                       help='ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--save_every', type=int, default=10,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°')
    
    # ë¡œê¹…
    parser.add_argument('--use_wandb', action='store_true', default=True,
                       help='Weights & Biases ì‚¬ìš©')
    parser.add_argument('--project_name', type=str, default='audiomanipulator',
                       help='W&B í”„ë¡œì íŠ¸ ì´ë¦„')
    
    # ë©€í‹°GPU ê´€ë ¨
    parser.add_argument('--num_gpus', type=int, default=1,
                       help='ì‚¬ìš©í•  GPU ìˆ˜ (0=CPU only)')
    parser.add_argument('--distributed', action='store_true',
                       help='ë¶„ì‚° í›ˆë ¨ ì‚¬ìš© (DistributedDataParallel)')
    
    args = parser.parse_args()
    
    # GPU ì„¤ì • í™•ì¸
    if args.num_gpus > 0 and not torch.cuda.is_available():
        print("âš ï¸  CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        args.num_gpus = 0
    
    available_gpus = torch.cuda.device_count()
    if args.num_gpus > available_gpus:
        print(f"âš ï¸  ìš”ì²­ëœ GPU ìˆ˜({args.num_gpus})ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜({available_gpus})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
        args.num_gpus = available_gpus
    
    # Weights & Biases ì´ˆê¸°í™” (ìµœì í™”ëœ ì„¤ì •)
    if args.use_wandb:
        try:
            print("ğŸš€ Wandb ì´ˆê¸°í™” ì¤‘...")
            wandb.init(
                project=args.project_name, 
                config=vars(args),  # argsë¥¼ dictë¡œ ë³€í™˜
                name=f"preset-gen-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                tags=['audio-processing', 'clap-loss', 'preset-generation'],
                # ì—…ë¡œë“œ ìµœì í™” ì„¤ì •
                settings=wandb.Settings(
                    _disable_stats=True,  # ì‹œìŠ¤í…œ í†µê³„ ë¹„í™œì„±í™”
                    _disable_meta=True,   # ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ë¹„í™œì„±í™”
                    console="off",        # ì½˜ì†” ë¡œê·¸ ì—…ë¡œë“œ ë¹„í™œì„±í™”
                    code_dir=None,        # ì½”ë“œ ì—…ë¡œë“œ ë¹„í™œì„±í™”
                )
            )
            print("âœ… Wandb ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"ğŸ“Š Project: {args.project_name}")
            print(f"ğŸ”— Run URL: {wandb.run.url if wandb.run else 'N/A'}")
                
        except Exception as e:
            print(f"âš ï¸  Wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("   ë¡œê¹… ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            args.use_wandb = False
    
    print("ğŸµ Audio Effect Preset Generation Training")
    print("=" * 50)
    print(f"ğŸ“ Data path: {args.data_path}")
    print(f"ğŸ¤– Text Encoder: {args.text_encoder_type}")
    print(f"ğŸ›ï¸  Model dimensions: text={args.text_embed_dim}, audio={args.audio_embed_dim}, hidden={args.hidden_dim}")
    print(f"ğŸš€ Training: {args.num_epochs} epochs, batch_size={args.batch_size}, lr={args.learning_rate}")
    
    if args.use_guide_presets:
        print(f"ğŸ¯ Guide Presets: ENABLED")
        print(f"   - Guide epochs: {args.guide_epochs}")
        print(f"   - Guide weight: {args.guide_weight}")
    else:
        print(f"ğŸ“ Pure Description Training: ENABLED (no guide presets)")
    
    print(f"ğŸ–¥ï¸  GPU ì„¤ì •: {args.num_gpus} GPUs, distributed={args.distributed}")
    
    # í›ˆë ¨ ì‹œì‘
    if args.num_gpus > 1 and args.distributed:
        # ë¶„ì‚° í›ˆë ¨
        print(f"ğŸš€ ë¶„ì‚° í›ˆë ¨ ì‹œì‘: {args.num_gpus} GPUs")
        mp.spawn(train_worker, args=(args.num_gpus, args), nprocs=args.num_gpus, join=True)
    else:
        # ë‹¨ì¼ GPU ë˜ëŠ” DataParallel í›ˆë ¨
        if args.num_gpus > 1:
            print(f"ğŸš€ DataParallel í›ˆë ¨ ì‹œì‘: {args.num_gpus} GPUs")
        elif args.num_gpus == 1:
            print("ğŸš€ ë‹¨ì¼ GPU í›ˆë ¨ ì‹œì‘")
        else:
            print("ğŸš€ CPU í›ˆë ¨ ì‹œì‘")
        
        trainer = TrainingManager(args, rank=0, world_size=1)
        trainer.train()
        
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì „ ì •ë¦¬
        if args.use_wandb:
            try:
                wandb.finish()
            except:
                pass


if __name__ == "__main__":
    main()
