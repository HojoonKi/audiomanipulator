#!/usr/bin/env python3

# HuggingFace tokenizers parallelism ê²½ê³  í•´ê²°
import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
import argparse
import random
import wandb
import traceback
import os

# HuggingFace tokenizers parallelism ê²½ê³  í•´ê²°
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

from torch.utils.data import DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm
import wandb
from datetime import datetime

# Import our custom modules
from dynamic_pipeline_factory import DynamicPipelineFactory
from dataset import (
    PresetDataset, 
    PureDescriptionDataset, 
    PretrainDataset,  # ì‚¬ì „ í›ˆë ¨ ì „ìš© ë°ì´í„°ì…‹ ì¶”ê°€
    create_custom_collate_fn, 
    load_descriptions, 
    split_descriptions
)

class TrainingManager:
    """í›ˆë ¨ ê´€ë¦¬ì - ë©€í‹°GPU ì§€ì›"""
    
    def __init__(self, args, rank=0, world_size=1):
        self.args = args
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')
        
        # ë¶„ì‚° í›ˆë ¨ ì´ˆê¸°í™”
        if world_size > 1:
            self.setup_distributed()
        
        # CLAP ë¡œë”© ì‹œ ì¶œë ¥ ì–µì œ
        import sys
        from contextlib import redirect_stdout, redirect_stderr
        import os
        
        if self.rank == 0:
            print("ğŸ”„ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... (CLAP ë¡œë”©)")
        
        # stdout/stderr ì„ì‹œ ì–µì œ (CLAP verbose ì¶œë ¥ ë°©ì§€)
        with open(os.devnull, 'w') as devnull:
            with redirect_stdout(devnull), redirect_stderr(devnull):
                # DynamicPipelineFactoryë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì´ˆê¸°í™”
                factory = DynamicPipelineFactory()
                
                # argsì—ì„œ text_encoder_type ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: sentence-transformer-large)
                text_encoder_type = getattr(args, 'text_encoder_type', 'sentence-transformer-large')
                
                # ë™ì  íŒŒì´í”„ë¼ì¸ ìƒì„± - ì°¨ì›ì€ ìë™ìœ¼ë¡œ ê²°ì •ë¨
                self.model = factory.create_pipeline(
                    encoder_preset=text_encoder_type,
                    use_clap=True,
                    backbone_type='residual',
                    decoder_type='parallel',
                    sample_rate=args.sample_rate,
                    target_params=500000
                )
                
                # ì‹¤ì œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì› ì—…ë°ì´íŠ¸ (ë™ì ìœ¼ë¡œ ê²°ì •ëœ ê°’)
                if hasattr(self.model, 'text_encoder'):
                    actual_text_dim = self.model.text_encoder.get_embedding_dim()
                    args.text_embed_dim = actual_text_dim  # args ì—…ë°ì´íŠ¸
                    if self.rank == 0:
                        print(f"ğŸ”§ ì‹¤ì œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {actual_text_dim}")
                        print(f"   ì„ íƒëœ ì¸ì½”ë”: {text_encoder_type}")
                
                # ë°±ë³¸ ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì„± í™•ì¸
                if hasattr(self.model, 'backbone'):
                    if hasattr(self.model.backbone, 'text_dim'):
                        backbone_text_dim = self.model.backbone.text_dim
                        if self.rank == 0:
                            print(f"ğŸ”§ ë°±ë³¸ í…ìŠ¤íŠ¸ ì…ë ¥ ì°¨ì›: {backbone_text_dim}")
                    if hasattr(self.model.backbone, 'clap_dim'):
                        backbone_clap_dim = self.model.backbone.clap_dim
                        if self.rank == 0:
                            print(f"ğŸ”§ ë°±ë³¸ CLAP ì…ë ¥ ì°¨ì›: {backbone_clap_dim}")
        
        # ëª…ì‹œì ìœ¼ë¡œ deviceë¡œ ì´ë™ (verbose ì¶œë ¥ì„ ìœ„í•´)
        if self.rank == 0:
            print(f"ğŸš€ ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™ ì¤‘...")
        self.model = self.model.to(self.device)
        
        # Pipeline ë‚´ë¶€ì˜ CLAP ëª¨ë“ˆì— ì ‘ê·¼ (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)
        self.clap_module = self.model.clap_encoder if hasattr(self.model, 'clap_encoder') else None
        
        # ë©€í‹°GPUìš© ëª¨ë¸ ë˜í•‘
        if world_size > 1:
            self.model = DDP(self.model, device_ids=[rank], output_device=rank)
            # CLAPì€ ì´ë¯¸ model ë‚´ë¶€ì— ìˆìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ë˜í•‘í•˜ì§€ ì•ŠìŒ
        elif torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            # CLAPì€ ì´ë¯¸ model ë‚´ë¶€ì— ìˆìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ë˜í•‘í•˜ì§€ ì•ŠìŒ
            
        # Optimizer - ì˜¤ì§ model parametersë§Œ (CLAPì€ frozen)
        model_params = list(self.model.parameters())
        trainable_params = [p for p in model_params if p.requires_grad]
        
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        # Alternative: ReduceLROnPlateau for adaptive learning rate
        # self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        #     self.optimizer, mode='min', factor=0.5, patience=10, verbose=True
        # )
        
        # Loss function
        self.criterion = nn.CosineEmbeddingLoss()
        
        if self.rank == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì¶œë ¥
            model_params = sum(p.numel() for p in self.model.parameters())
            model_trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_trainable = model_trainable
            
            print(f"ğŸš€ í›ˆë ¨ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - Device: {self.device}")
            print(f"   - World size: {world_size}")
            print(f"   - Model parameters: {model_params:,} ({model_trainable:,} trainable)")
            print(f"   - Total trainable: {total_trainable:,}")
            print(f"   - CLAP ì‚¬ìš©: {'âœ… Enabled (ë‚´ì¥)' if self.clap_module else 'âŒ Disabled'}")
            
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(self.device) / 1024**3  # GB
                reserved = torch.cuda.memory_reserved(self.device) / 1024**3   # GB
                max_memory = torch.cuda.get_device_properties(self.device).total_memory / 1024**3  # GB
                
                print(f"\nğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
                print(f"   - Allocated: {allocated:.2f} GB")
                print(f"   - Reserved: {reserved:.2f} GB") 
                print(f"   - Total GPU Memory: {max_memory:.2f} GB")
                print(f"   - Usage: {(allocated/max_memory)*100:.1f}% allocated, {(reserved/max_memory)*100:.1f}% reserved")
                
                # ëª¨ë¸ì´ ì‹¤ì œë¡œ GPUì— ìˆëŠ”ì§€ í™•ì¸
                model_on_gpu = next(self.model.parameters()).device
                print(f"   - Model Device: {model_on_gpu}")
            
            print("=" * 60)
    

    
    def setup_distributed(self):
        """ë¶„ì‚° í›ˆë ¨ ì„¤ì •"""
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        dist.init_process_group(
            backend='nccl',
            rank=self.rank,
            world_size=self.world_size
        )
    
    def load_datasets(self):
        """ë°ì´í„°ì…‹ ë¡œë“œ - ë¶„ì‚° í›ˆë ¨ ì§€ì›"""
        # dataset.pyì˜ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ descriptions ë¡œë“œ
        descriptions = load_descriptions(
            data_path=self.args.data_path,
            use_sampled_descriptions=self.args.use_sampled_descriptions,
            max_descriptions=self.args.max_descriptions
        )
        
        if not descriptions:
            if self.rank == 0:
                print("âŒ Description ë¡œë“œ ì‹¤íŒ¨")
            return None, None
        
        # ë°ì´í„°ì…‹ ë¶„í• 
        train_descriptions, val_descriptions = split_descriptions(descriptions, train_ratio=0.8)
        
        if self.rank == 0:
            print(f"ğŸ“š ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
            print(f"   - í›ˆë ¨: {len(train_descriptions)}ê°œ")
            print(f"   - ê²€ì¦: {len(val_descriptions)}ê°œ")
        
        # Fine preset ê²½ë¡œ
        fine_preset_path = os.path.join(self.args.data_path, 'descriptions', 'fined_presets_filtered.py')
        
        # Custom collate function ìƒì„± (dataset.py í•¨ìˆ˜ ì‚¬ìš©)
        custom_collate_fn = create_custom_collate_fn(include_guide_preset=self.args.use_guide_presets)
        
        # ë°ì´í„°ì…‹ ìƒì„± - ì‚¬ì „ í›ˆë ¨ ì—¬ë¶€ì™€ ê°€ì´ë“œ í”„ë¦¬ì…‹ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì„ íƒ
        if self.args.use_guide_presets:
            # Guide preset ì‚¬ìš© (PresetDataset)
            train_dataset = PresetDataset(
                descriptions=train_descriptions,
                audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
                use_fine_tuned_presets=True,
                fine_preset_path=fine_preset_path,
                sample_rate=self.args.sample_rate,
                audio_length=self.args.audio_length
            )
            
            val_dataset = PresetDataset(
                descriptions=val_descriptions,
                audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
                use_fine_tuned_presets=False,  # ê²€ì¦ì—ì„œëŠ” guide preset ì‚¬ìš© ì•ˆí•¨
                sample_rate=self.args.sample_rate,
                audio_length=self.args.audio_length
            )
        else:
            # Pure Description ì „ìš© (PureDescriptionDataset)
            train_dataset = PureDescriptionDataset(
                descriptions=train_descriptions,
                audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
                sample_rate=self.args.sample_rate,
                audio_length=self.args.audio_length
            )
            
            val_dataset = PureDescriptionDataset(
                descriptions=val_descriptions,
                audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
                sample_rate=self.args.sample_rate,
                audio_length=self.args.audio_length
            )
        
        # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì„¤ì •
        train_sampler = None
        val_sampler = None
        if self.world_size > 1:
            train_sampler = DistributedSampler(
                train_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
            val_sampler = DistributedSampler(
                val_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False
            )
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.args.batch_size,
            shuffle=(train_sampler is None),
            sampler=train_sampler,
            num_workers=self.args.num_workers,
            pin_memory=True,
            collate_fn=custom_collate_fn  # Use custom collate function
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            sampler=val_sampler,
            num_workers=self.args.num_workers,
            pin_memory=True,
            collate_fn=custom_collate_fn  # Use custom collate function
        )
        
        return train_loader, val_loader
    
    def compute_clap_loss_batch_wrapper(self, processed_audio_batch, target_descriptions):
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ - ê¸°ì¡´ compute_clap_loss ì‚¬ìš©"""
        try:
            # ê¸°ì¡´ compute_clap_lossëŠ” ì´ë¯¸ ë°°ì¹˜ë¥¼ ì§€ì›í•¨
            return self.compute_clap_loss(processed_audio_batch, target_descriptions)
        except Exception as e:
            print(f"âŒ CLAP batch wrapper ì‹¤íŒ¨: {e}")
            # fallback
            dummy_param = next(self.model.parameters())
            return torch.mean(dummy_param * 0.0) + 0.1

    def compute_clap_loss(self, processed_audio, target_description):
        """CLAP ê¸°ë°˜ ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ loss ê³„ì‚° (gradient ìœ ì§€)"""
        try:
            # Pipeline ë‚´ë¶€ì˜ CLAP ëª¨ë“ˆ ì ‘ê·¼ (DDP ê³ ë ¤)
            if hasattr(self.model, 'module'):
                # DDP wrapped model
                clap_module = getattr(self.model.module, 'clap_encoder', None)
            else:
                # Normal model
                clap_module = getattr(self.model, 'clap_encoder', None)
            
            if clap_module is None:
                print("âš ï¸  CLAP ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, fallback loss ì‚¬ìš©")
                return torch.tensor(1.0, device=self.device, requires_grad=True)
            
            # ë‹¨ì¼ description ì²˜ë¦¬
            if isinstance(target_description, list) and len(target_description) == 1:
                target_description = target_description[0]
            
            # Processed audioê°€ listì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì„ íƒí•˜ê±°ë‚˜ í…ì„œë¡œ ë³€í™˜
            if isinstance(processed_audio, list):
                if len(processed_audio) > 0:
                    processed_audio = processed_audio[0]  # ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                else:
                    print("âš ï¸  Processed audio listê°€ ë¹„ì–´ìˆìŒ")
                    return torch.tensor(1.0, device=self.device, requires_grad=True)
            
            # í…ì„œê°€ ì•„ë‹Œ ê²½ìš° í…ì„œë¡œ ë³€í™˜
            if not isinstance(processed_audio, torch.Tensor):
                print(f"âš ï¸  Processed audioê°€ í…ì„œê°€ ì•„ë‹˜: {type(processed_audio)}")
                return torch.tensor(1.0, device=self.device, requires_grad=True)
            
            # Processed audio ì°¨ì› ì²˜ë¦¬
            if processed_audio.dim() == 3:  # [batch, channels, samples]
                processed_audio = processed_audio.squeeze(1)  # [batch, samples]
            elif processed_audio.dim() == 1:  # [samples]
                processed_audio = processed_audio.unsqueeze(0)  # [1, samples]
            
            # CLAP loss ê³„ì‚° (ë‹¨ìˆœíˆ embedding ë¹„êµ)
            clap_loss = clap_module.compute_clap_loss(processed_audio, target_description)
            
            # CLAP lossê°€ gradientë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
            if not clap_loss.requires_grad:
                print(f"âš ï¸ CLAP lossê°€ gradientë¥¼ ê°€ì§€ì§€ ì•ŠìŒ, ë”ë¯¸ loss ìƒì„±")
                # gradientê°€ ìˆëŠ” ë”ë¯¸ loss ìƒì„± (ëª¨ë¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                dummy_param = next(self.model.parameters())
                clap_loss = torch.mean(dummy_param * 0.0) + 0.1  # ëª¨ë¸ê³¼ ì—°ê²°ëœ ë”ë¯¸ loss
            
            return clap_loss
            
        except Exception as e:
            print(f"âŒ CLAP loss ê³„ì‚° ì‹¤íŒ¨: {e}")
            # gradientê°€ ìˆëŠ” ë”ë¯¸ loss ìƒì„±
            try:
                dummy_param = next(self.model.parameters())
                fallback_loss = torch.mean(dummy_param * 0.0) + 0.1
                return fallback_loss
            except:
                return torch.tensor(1.0, device=self.device, requires_grad=True)
    
    
    def train_epoch(self, train_loader, epoch):
        """í•œ ì—í¬í¬ í›ˆë ¨ - Pure Description Training Only"""
        self.model.train()
        total_loss = 0.0
        
        # í›ˆë ¨ ëª¨ë“œ: í•­ìƒ Pure Description
        training_mode = "Text Descriptions"
        
        # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì—í¬í¬ ì„¤ì •
        if hasattr(train_loader.sampler, 'set_epoch'):
            train_loader.sampler.set_epoch(epoch)
        
        if self.rank == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ progress bar í‘œì‹œ
            desc = f"E{epoch+1}/{self.args.num_epochs} (Desc)"
            pbar = tqdm(train_loader, desc=desc, leave=False)  # leave=Falseë¡œ progress bar ì§€ìš°ê¸°
        else:
            pbar = train_loader
        
        for batch_idx, batch in enumerate(pbar):
            try:
                descriptions = batch['description']
                audios = batch['audio'].to(self.device, non_blocking=True)
                
                # Audio tensor ì°¨ì› í™•ì¸ ë° ìˆ˜ì •
                if audios.dim() == 2:  # [batch_size, samples]
                    audios = audios.unsqueeze(1)  # [batch_size, 1, samples] -> mono ì±„ë„ ì¶”ê°€
                
                # Pure Description Training: ëª¨ë“  í•­ëª©ì„ descriptionìœ¼ë¡œ ì²˜ë¦¬
                try:
                    # ì „ì²´ ë°°ì¹˜ë¥¼ í•œë²ˆì— ì²˜ë¦¬
                    self.model.train()
                    outputs = self.model(
                        texts=descriptions,
                        audio=audios,
                        use_real_audio=False
                    )
                    
                    # ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆì— CLAP loss ê³„ì‚°
                    if 'processed_audio' in outputs:
                        processed_audio_batch = outputs['processed_audio']
                    else:
                        processed_audio_batch = audios
                    
                    # CLAP lossë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³„ì‚°
                    batch_loss = self.compute_clap_loss_batch_wrapper(processed_audio_batch, descriptions)
                    valid_items = len(descriptions)
                
                except Exception as e:
                    print(f"âŒ Description batch ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    # gradientê°€ ìˆëŠ” ë”ë¯¸ loss
                    dummy_param = next(self.model.parameters())
                    batch_loss = torch.mean(dummy_param * 0.0) + 0.1
                    valid_items = 1  # ë”ë¯¸ í•­ëª© 1ê°œë¡œ ì„¤ì •
                
                # Valid itemsê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if valid_items == 0:
                    if self.rank == 0:
                        print(f"âš ï¸ ë°°ì¹˜ {batch_idx}: ì²˜ë¦¬í•  í•­ëª© ì—†ìŒ")
                    continue
                
                # ë°°ì¹˜ ì²˜ë¦¬ì—ì„œëŠ” ì´ë¯¸ í‰ê·  lossê°€ ê³„ì‚°ë¨
                # batch_loss = batch_loss / valid_items  # ì œê±°
                
                # batch_lossê°€ gradientë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
                if not batch_loss.requires_grad:
                    if self.rank == 0:
                        print(f"âš ï¸ ë°°ì¹˜ {batch_idx}: Lossê°€ gradientë¥¼ ê°€ì§€ì§€ ì•ŠìŒ, ê±´ë„ˆë›°ê¸°")
                    continue
                
                # Backward pass
                self.optimizer.zero_grad()
                batch_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += batch_loss.item()
                
                # Progress bar ì—…ë°ì´íŠ¸ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
                if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                    # í˜„ì¬ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°
                    current_lr = self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.args.learning_rate
                    
                    pbar.set_postfix({
                        'Loss': f'{batch_loss.item():.4f}',
                        'AvgLoss': f'{total_loss/(batch_idx+1):.4f}',
                        'Items': f'{valid_items}/{len(descriptions)}',
                        'LR': f'{current_lr:.1e}',
                        'Mode': training_mode[:5]  # "Guide" or "Text "
                    })
                    
                    # wandb logging (ë°°ì¹˜ë³„) - 50ë²ˆë§ˆë‹¤ë§Œ ë¡œê¹…
                    if self.args.use_wandb and batch_idx % 50 == 0:
                        # ë°°ì¹˜ë³„ ë¡œê¹…ì€ ê¸€ë¡œë²Œ stepìœ¼ë¡œ ê³„ì‚°
                        global_step = epoch * len(train_loader) + batch_idx
                        wandb.log({
                            'batch_loss': batch_loss.item(),
                            'batch_idx': batch_idx,
                            'epoch': epoch + 1,
                            'training_mode': training_mode,
                            'learning_rate': self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.args.learning_rate
                        }, step=global_step)
                    
            except Exception as e:
                if self.rank == 0:
                    print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                continue
        
        # ë¶„ì‚° í›ˆë ¨ì—ì„œ loss í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            avg_loss = torch.tensor(total_loss / len(train_loader)).to(self.device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss / self.world_size
            final_loss = avg_loss.item()
        else:
            final_loss = total_loss / max(1, len(train_loader))
        
        # wandb logging (ì—í¬í¬ë³„)
        if self.rank == 0 and self.args.use_wandb:
            wandb.log({
                'train_loss_epoch': final_loss,
                'training_mode': training_mode
            }, step=epoch)
        
        return final_loss
    
    def compute_guide_loss(self, generated_preset, guide_preset):
        """Guide presetê³¼ì˜ ì°¨ì´ë¥¼ ì´ìš©í•œ ê°„ë‹¨í•œ MSE loss"""
        try:
            # ë””ì½”ë”ì—ì„œ _raw_params ì§ì ‘ ì¶”ì¶œ
            if isinstance(generated_preset, dict) and "_raw_params" in generated_preset:
                generated_tensor = generated_preset["_raw_params"].to(self.device)
                
                # Guide presetì„ ê°„ë‹¨í•œ tensorë¡œ ë³€í™˜
                guide_values = self.extract_guide_values(guide_preset)
                if guide_values is None:
                    return self.create_dummy_loss()
                
                guide_tensor = torch.FloatTensor(guide_values).to(self.device)
                
                # ë°°ì¹˜ ì°¨ì› ì²˜ë¦¬
                if generated_tensor.dim() == 2:
                    generated_tensor = generated_tensor.squeeze(0)
                
                # ì§ì ‘ MSE ê³„ì‚°
                mse_loss = nn.MSELoss()(generated_tensor, guide_tensor)
                
                # ì²« ë²ˆì§¸ ë””ë²„ê¹…
                if self.rank == 0 and hasattr(self, '_first_guide_loss'):
                    print(f"ğŸ“Š Guide Loss: {mse_loss.item():.6f}")
                    print(f"   Generated: {generated_tensor.shape} [{generated_tensor.min():.3f}, {generated_tensor.max():.3f}]")
                    print(f"   Guide: {guide_tensor.shape} [{guide_tensor.min():.3f}, {guide_tensor.max():.3f}]")
                    del self._first_guide_loss
                
                return mse_loss
            else:
                if self.rank == 0:
                    print(f"âš ï¸ _raw_params ì—†ìŒ")
                return self.create_dummy_loss()
                
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ Guide loss ì‹¤íŒ¨: {e}")
            return self.create_dummy_loss()
    
    def compute_batch_guide_loss(self, batch_generated_preset, batch_guide_presets):
        """ë°°ì¹˜ ë‹¨ìœ„ Guide Loss ê³„ì‚° - ì‚¬ì „ ê²€ì¦ëœ preset ì‚¬ìš©"""
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ _raw_params ì¶”ì¶œ
            if isinstance(batch_generated_preset, dict) and "_raw_params" in batch_generated_preset:
                generated_batch_tensor = batch_generated_preset["_raw_params"].to(self.device)
                # generated_batch_tensor: [batch_size, 28] ë˜ëŠ” [batch_size, 1, 28]
                
                if generated_batch_tensor.dim() == 3:
                    generated_batch_tensor = generated_batch_tensor.squeeze(1)  # [batch_size, 28]
                
                # ğŸ”¥ ê°„ì†Œí™”: ëª¨ë“  presetì´ ì´ë¯¸ ê²€ì¦ë¨, ì§ì ‘ ë³€í™˜
                batch_guide_values = []
                
                for guide_preset in batch_guide_presets:
                    guide_values = self.extract_guide_values(guide_preset)
                    batch_guide_values.append(guide_values)
                
                # Guide valuesë¥¼ ë°°ì¹˜ í…ì„œë¡œ ìŠ¤íƒ
                guide_batch_tensor = torch.FloatTensor(batch_guide_values).to(self.device)  # [batch_size, 28]
                
                # ë°°ì¹˜ MSE loss ê³„ì‚°
                batch_mse_loss = nn.MSELoss()(generated_batch_tensor, guide_batch_tensor)
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ ë””ë²„ê¹…
                if self.rank == 0 and hasattr(self, '_first_guide_loss'):
                    print(f"ğŸ“Š Batch Guide Loss: {batch_mse_loss.item():.6f}")
                    print(f"   Generated Batch: {generated_batch_tensor.shape} [{generated_batch_tensor.min():.3f}, {generated_batch_tensor.max():.3f}]")
                    print(f"   Guide Batch: {guide_batch_tensor.shape} [{guide_batch_tensor.min():.3f}, {guide_batch_tensor.max():.3f}]")
                    print(f"   All presets pre-validated âœ…")
                    del self._first_guide_loss
                
                return batch_mse_loss
                
            else:
                
                if self.rank == 0:
                    print(f"âš ï¸ ë°°ì¹˜ì—ì„œ _raw_params ì—†ìŒ")
                return self.create_dummy_loss()
                
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ Batch guide loss ì‹¤íŒ¨: {e}")
            return self.create_dummy_loss()
    
    def fallback_individual_guide_processing(self, descriptions, audios, guide_presets):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ê°œë³„ ì²˜ë¦¬ fallback - ì‚¬ì „ ê²€ì¦ëœ preset ì‚¬ìš©"""
        try:
            total_loss = 0.0
            
            for desc, audio, guide_preset in zip(descriptions, audios, guide_presets):
                try:
                    single_audio = audio.unsqueeze(0)  # [1, channels, samples]
                    
                    outputs = self.model(
                        texts=[desc],
                        audio=single_audio,
                        use_real_audio=False
                    )
                    
                    if 'preset_params' not in outputs:
                        continue
                        
                    predicted_params = outputs['preset_params']
                    guide_loss = self.compute_guide_loss(predicted_params, guide_preset)
                    
                    # ì‚¬ì „ ê²€ì¦ëœ presetì´ë¯€ë¡œ ê°„ë‹¨í•œ ê²€ì¦ë§Œ
                    if guide_loss.requires_grad:
                        total_loss += guide_loss
                        
                except Exception:
                    continue
            
            # ëª¨ë“  presetì´ ìœ íš¨í•˜ë¯€ë¡œ valid_items = len(guide_presets)
            return total_loss / len(guide_presets) if len(guide_presets) > 0 else self.create_dummy_loss()
            
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ Fallback ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self.create_dummy_loss()
    
    def extract_guide_values(self, guide_preset):
        """Guide presetì—ì„œ 28ê°œ ê°’ ì¶”ì¶œ - ì‚¬ì „ ê²€ì¦ëœ preset ì‚¬ìš©"""
        # ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ ê²€ì¦ë¨ - ë°”ë¡œ ê°’ ì¶”ì¶œ
        values = []
        
        # EQ (20ê°œ): center_freq, gain_db, q, filter_type Ã— 5
        eq_section = guide_preset['eq']
        for i in range(1, 6):  # band_1 ~ band_5
            band = eq_section[f'band_{i}']
            values.extend([
                band['center_freq'],
                band['gain_db'], 
                band['q'],
                band['filter_type']
            ])
        
        # Reverb (5ê°œ)
        reverb = guide_preset['reverb']
        values.extend([
            reverb['room_size'],
            reverb['pre_delay'], 
            reverb['diffusion'],
            reverb['damping'],
            reverb['wet_gain']
        ])
        
        # Distortion (2ê°œ)
        dist = guide_preset['distortion']
        values.extend([
            dist['gain'],
            dist['color']
        ])
        
        # Pitch (1ê°œ)
        pitch = guide_preset['pitch']
        values.append(pitch['scale'])
        
        return values  # 28ê°œ ê°’ ë³´ì¥ë¨
    
    def create_dummy_loss(self):
        """ê°„ë‹¨í•œ ë”ë¯¸ loss"""
        dummy_param = next(self.model.parameters())
        return torch.mean(dummy_param * 0.0) + 0.001
    
    def validate(self, val_loader):
        """ê²€ì¦ - ë¶„ì‚° í›ˆë ¨ ì§€ì›"""
        self.model.eval()
        total_loss = 0.0
        
        pbar = val_loader
        if self.rank == 0:
            pbar = tqdm(val_loader, desc="Validation", leave=False)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(pbar):
                descriptions = batch['description']
                audios = batch['audio'].to(self.device, non_blocking=True)
                
                # Forward pass through pipeline (eval modeë¡œ ë¹ ë¥´ê²Œ)
                self.model.eval()  # ê²€ì¦ì—ì„œëŠ” eval ëª¨ë“œ ì‚¬ìš©
                outputs = self.model(
                    texts=descriptions,
                    audio=audios,
                    use_real_audio=False
                )
                
                # ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ í•œ ë²ˆì— CLAP loss ê³„ì‚°
                if 'processed_audio' in outputs:
                    processed_audio_batch = outputs['processed_audio']
                else:
                    processed_audio_batch = audios
                
                # CLAP lossë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê³„ì‚° (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
                batch_loss = self.compute_clap_loss(processed_audio_batch, descriptions)
                total_loss += batch_loss.item()
                
                # Progress bar ì—…ë°ì´íŠ¸ (validation)
                if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                    pbar.set_postfix({
                        'ValLoss': f'{batch_loss.item():.4f}',
                        'AvgValLoss': f'{total_loss/(batch_idx+1):.4f}',
                        'Samples': len(descriptions)
                    })
        
        # ë¶„ì‚° í›ˆë ¨ì—ì„œ validation loss í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            avg_loss = torch.tensor(total_loss / len(val_loader)).to(self.device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss / self.world_size
            final_val_loss = avg_loss.item()
        else:
            final_val_loss = total_loss / len(val_loader)
        
        return final_val_loss
    
    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ"""
        if self.rank != 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì•„ë‹ˆë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
            return
            
        # DDP ëª¨ë¸ì˜ ê²½ìš° module ì ‘ê·¼ í•„ìš”
        model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'args': self.args
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸
        checkpoint_path = os.path.join(self.args.save_dir, f'checkpoint_epoch_{epoch}.pt')
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸
        if is_best:
            best_path = os.path.join(self.args.save_dir, 'best_model.pt')
            torch.save(checkpoint, best_path)
    
    def cleanup_distributed(self):
        """ë¶„ì‚° í›ˆë ¨ ì •ë¦¬"""
        if self.world_size > 1:
            dist.destroy_process_group()
    
    def simple_pretrain(self):
        """ê°„ë‹¨í•œ ì‚¬ì „ í›ˆë ¨ - Fine Presetë§Œ ì‚¬ìš© (Description ì—†ìŒ)"""
        if not self.args.enable_pretrain:
            return
        
        if self.rank == 0:
            print("\n" + "="*60)
            print("ğŸ¯ ì‚¬ì „ í›ˆë ¨ ì‹œì‘ (Fine Preset Only - No Descriptions)")
            print("="*60)
            print(f"   - ì‚¬ì „ í›ˆë ¨ ì—í¬í¬: {self.args.pretrain_epochs}")
            print(f"   - ì‚¬ì „ í›ˆë ¨ í•™ìŠµë¥ : {self.args.pretrain_lr}")
            print(f"   - ëª¨ë“œ: Fine Preset íŒŒë¼ë¯¸í„° ë§¤ì¹­ë§Œ")
        
        # Fine preset ê²½ë¡œ
        fine_preset_path = os.path.join(self.args.data_path, 'descriptions', 'fined_presets_filtered.py')
        
        # ê¸°ì¡´ ì˜µí‹°ë§ˆì´ì € ë°±ì—…  
        original_optimizer = self.optimizer
        original_lr = self.args.learning_rate
        
        # ì‚¬ì „ í›ˆë ¨ìš© ì˜µí‹°ë§ˆì´ì € ìƒì„±
        pretrain_optimizer = optim.AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=self.args.pretrain_lr,
            weight_decay=self.args.weight_decay
        )
        self.optimizer = pretrain_optimizer
        self.args.learning_rate = self.args.pretrain_lr
        
        try:
            # ì‚¬ì „ í›ˆë ¨ ì „ìš© ë°ì´í„°ì…‹ ë¡œë“œ (Fine Presetë§Œ)
            train_dataset = PretrainDataset(
                fine_preset_path=fine_preset_path,
                audio_dataset_path=os.path.join(self.args.data_path, 'audio_dataset'),
                sample_rate=self.args.sample_rate,
                audio_length=self.args.audio_length
            )
            
            # ê²€ì¦ìš©ìœ¼ë¡œëŠ” ì ì€ ìˆ˜ì˜ ìƒ˜í”Œë§Œ ì‚¬ìš©
            val_size = min(100, len(train_dataset) // 10)  # 10%ì™€ 100ê°œ ì¤‘ ì‘ì€ ê²ƒ
            val_indices = random.sample(range(len(train_dataset)), val_size)
            train_indices = [i for i in range(len(train_dataset)) if i not in val_indices]
            
            train_subset = torch.utils.data.Subset(train_dataset, train_indices)
            val_subset = torch.utils.data.Subset(train_dataset, val_indices)
            
            # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì„¤ì •
            train_sampler = None
            val_sampler = None
            if self.world_size > 1:
                from torch.utils.data.distributed import DistributedSampler
                train_sampler = DistributedSampler(
                    train_subset,
                    num_replicas=self.world_size,
                    rank=self.rank,
                    shuffle=True
                )
                val_sampler = DistributedSampler(
                    val_subset,
                    num_replicas=self.world_size,
                    rank=self.rank,
                    shuffle=False
                )
            
            # Custom collate function (guide preset í¬í•¨)
            custom_collate_fn = create_custom_collate_fn(include_guide_preset=True)
            
            # ì‚¬ì „ í›ˆë ¨ ì „ìš© ë°°ì¹˜ í¬ê¸° ì‚¬ìš© (ì„¤ì •ë˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©)
            pretrain_batch_size = self.args.pretrain_batch_size if self.args.pretrain_batch_size is not None else self.args.batch_size
            
            train_loader = DataLoader(
                train_subset,
                batch_size=pretrain_batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=self.args.num_workers,
                pin_memory=True,
                collate_fn=custom_collate_fn
            )
            
            val_loader = DataLoader(
                val_subset,
                batch_size=pretrain_batch_size,
                shuffle=False,
                sampler=val_sampler,
                num_workers=self.args.num_workers,
                pin_memory=True,
                collate_fn=custom_collate_fn
            )
            
            if self.rank == 0:
                print(f"ğŸ¯ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°:")
                print(f"   - í›ˆë ¨ ìƒ˜í”Œ: {len(train_subset)}")
                print(f"   - ê²€ì¦ ìƒ˜í”Œ: {len(val_subset)}")
                print(f"   - ì‚¬ì „ í›ˆë ¨ ë°°ì¹˜ í¬ê¸°: {pretrain_batch_size}")
                print(f"   - ì¼ë°˜ í›ˆë ¨ ë°°ì¹˜ í¬ê¸°: {self.args.batch_size}")
            
            # ì‚¬ì „ í›ˆë ¨ ë£¨í”„ - Guide Preset íŒŒë¼ë¯¸í„° ë§¤ì¹­ë§Œ ìˆ˜í–‰
            best_pretrain_loss = float('inf')
            
            for epoch in range(self.args.pretrain_epochs):
                # ë¶„ì‚° ìƒ˜í”ŒëŸ¬ ì—í¬í¬ ì„¤ì •
                if hasattr(train_loader.sampler, 'set_epoch'):
                    train_loader.sampler.set_epoch(epoch)
                
                # ì‚¬ì „ í›ˆë ¨ ì—í¬í¬ (Guide Preset ë§¤ì¹­ë§Œ)
                train_loss = self.pretrain_epoch(train_loader, epoch)
                
                # ê°„ë‹¨í•œ ê²€ì¦
                val_loss = self.pretrain_validate(val_loader)
                
                if self.rank == 0:
                    print(f"Pretrain {epoch+1}/{self.args.pretrain_epochs}: "
                          f"Loss={train_loss:.6f}, Val={val_loss:.6f} (Guide Preset Only)")
                    
                    # Wandb ì‚¬ì „ í›ˆë ¨ ë¡œê¹…
                    if self.args.use_wandb:
                        wandb.log({
                            'pretrain_epoch': epoch + 1,
                            'pretrain_train_loss': train_loss,
                            'pretrain_val_loss': val_loss,
                            'pretrain_lr': self.args.pretrain_lr,
                            'phase': 'pretrain_guide_only'
                        }, step=epoch)
                
                # ìµœê³  ì„±ëŠ¥ ì¶”ì 
                if val_loss < best_pretrain_loss:
                    best_pretrain_loss = val_loss
                    
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                    if self.rank == 0:
                        pretrain_dir = os.path.join(self.args.save_dir, 'pretrain')
                        os.makedirs(pretrain_dir, exist_ok=True)
                        
                        model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()
                        torch.save({
                            'model_state_dict': model_state,
                            'epoch': epoch,
                            'pretrain_loss': val_loss,
                            'args': self.args
                        }, os.path.join(pretrain_dir, 'best_pretrain.pt'))
                        
                        # Wandbì— ìµœê³  ì„±ëŠ¥ ê¸°ë¡
                        if self.args.use_wandb:
                            wandb.log({
                                'pretrain_best_loss': best_pretrain_loss,
                                'pretrain_best_epoch': epoch + 1
                            }, step=epoch)
            
            if self.rank == 0:
                print(f"âœ… ì‚¬ì „ í›ˆë ¨ ì™„ë£Œ! ìµœê³  ì„±ëŠ¥: {best_pretrain_loss:.6f}")
                print("   ì‚¬ì „ í›ˆë ¨: Fine Preset íŒŒë¼ë¯¸í„° ë§¤ì¹­ë§Œ ìˆ˜í–‰ë¨")
                print("="*60)
        
        except Exception as e:
            if self.rank == 0:
                print(f"âŒ ì‚¬ì „ í›ˆë ¨ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        finally:
            # ì›ë˜ ì„¤ì • ë³µì›
            self.args.learning_rate = original_lr
            self.optimizer = original_optimizer

    def pretrain_epoch(self, train_loader, epoch):
        """ì‚¬ì „ í›ˆë ¨ ì „ìš© ì—í¬í¬ - Guide Preset íŒŒë¼ë¯¸í„° ë§¤ì¹­ë§Œ (ë°°ì¹˜ ìµœì í™”)"""
        self.model.train()
        total_loss = 0.0
        
        # ë””ë²„ê¹… í”Œë˜ê·¸ ì„¤ì • (ì²« ë²ˆì§¸ ì—í¬í¬ì—ì„œë§Œ)
        if epoch == 0:
            self._first_preset_conversion = True
            self._first_guide_loss = True
        
        if self.rank == 0:
            desc = f"Pretrain E{epoch+1}/{self.args.pretrain_epochs} (Guide Only)"
            pbar = tqdm(train_loader, desc=desc, leave=False)
        else:
            pbar = train_loader
        
        for batch_idx, batch in enumerate(pbar):
            try:
                descriptions = batch['description']  # ë”ë¯¸ description
                audios = batch['audio'].to(self.device, non_blocking=True)
                guide_presets = batch.get('guide_preset', [])
                
                # Audio tensor ì°¨ì› í™•ì¸ ë° ìˆ˜ì •
                if audios.dim() == 2:
                    audios = audios.unsqueeze(1)
                
                # ğŸ”¥ ë°°ì¹˜ ìµœì í™”: ìœ íš¨í•œ guide presetì´ ìˆëŠ” í•­ëª©ë§Œ í•„í„°ë§ (ê°„ì†Œí™”)
                valid_indices = []
                valid_descriptions = []
                valid_audios = []
                valid_guide_presets = []
                
                for i, guide_preset in enumerate(guide_presets):
                    # ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ ê²€ì¦ë¨ - ë¹ˆ ë”•ì…”ë„ˆë¦¬ë§Œ í™•ì¸
                    if guide_preset:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ìœ íš¨
                        valid_indices.append(i)
                        valid_descriptions.append(descriptions[i])
                        valid_audios.append(audios[i])
                        valid_guide_presets.append(guide_preset)
                
                if len(valid_indices) == 0:
                    # ìœ íš¨í•œ guide presetì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    continue
                
                # ğŸš€ ë°°ì¹˜ ì²˜ë¦¬: ìœ íš¨í•œ í•­ëª©ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬
                try:
                    # ìœ íš¨í•œ ì˜¤ë””ì˜¤ë“¤ì„ ë°°ì¹˜ í…ì„œë¡œ ìŠ¤íƒ
                    valid_audio_batch = torch.stack(valid_audios)  # [valid_batch_size, channels, samples]
                    
                    # ëª¨ë¸ forward pass - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                    outputs = self.model(
                        texts=valid_descriptions,
                        audio=valid_audio_batch,
                        use_real_audio=False
                    )
                    
                    if 'preset_params' not in outputs:
                        if self.rank == 0:
                            print(f"âš ï¸ ë°°ì¹˜ {batch_idx}: preset_paramsê°€ ì¶œë ¥ì— ì—†ìŒ")
                        continue
                    
                    # ğŸ¯ ë°°ì¹˜ Guide Loss ê³„ì‚°
                    batch_loss = self.compute_batch_guide_loss(outputs['preset_params'], valid_guide_presets)
                    
                    # Loss ìœ íš¨ì„± ê²€ì¦
                    if not batch_loss.requires_grad or torch.isnan(batch_loss) or torch.isinf(batch_loss):
                        if self.rank == 0:
                            print(f"âš ï¸ ë°°ì¹˜ {batch_idx}: ë¬´íš¨í•œ batch guide loss")
                        continue
                    
                    # Backward pass
                    self.optimizer.zero_grad()
                    batch_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    
                    total_loss += batch_loss.item()
                    
                    # Progress bar ì—…ë°ì´íŠ¸
                    if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                        pbar.set_postfix({
                            'GuideLoss': f'{batch_loss.item():.4f}',
                            'AvgLoss': f'{total_loss/(batch_idx+1):.4f}',
                            'ValidItems': f'{len(valid_indices)}/{len(descriptions)}',
                            'BatchEff': f'{len(valid_indices)}/{len(descriptions)*100:.0f}%',
                            'Mode': 'Guide'
                        })
                
                except Exception as e:
                    if self.rank == 0:
                        print(f"âŒ ë°°ì¹˜ forward pass ì‹¤íŒ¨: {e}")
                    # Fallback: ê°œë³„ ì²˜ë¦¬
                    batch_loss = self.fallback_individual_guide_processing(
                        valid_descriptions, valid_audios, valid_guide_presets
                    )
                    if batch_loss is not None and batch_loss.requires_grad:
                        self.optimizer.zero_grad()
                        batch_loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                        self.optimizer.step()
                        total_loss += batch_loss.item()
                    
            except Exception as e:
                if self.rank == 0:
                    print(f"âŒ ì‚¬ì „ í›ˆë ¨ ë°°ì¹˜ {batch_idx} ì‹¤íŒ¨: {e}")
                continue
        
        # ë¶„ì‚° í›ˆë ¨ì—ì„œ loss í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            avg_loss = torch.tensor(total_loss / len(train_loader)).to(self.device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss / self.world_size
            final_loss = avg_loss.item()
        else:
            final_loss = total_loss / max(1, len(train_loader))
        
        return final_loss

    def pretrain_validate(self, val_loader):
        """ì‚¬ì „ í›ˆë ¨ ì „ìš© ê²€ì¦ - Guide Preset ë§¤ì¹­ë§Œ (ë°°ì¹˜ ìµœì í™”)"""
        self.model.eval()
        total_loss = 0.0
        
        pbar = val_loader
        if self.rank == 0:
            pbar = tqdm(val_loader, desc="Pretrain Validation", leave=False)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(pbar):
                try:
                    descriptions = batch['description']
                    audios = batch['audio'].to(self.device, non_blocking=True)
                    guide_presets = batch.get('guide_preset', [])
                    
                    if audios.dim() == 2:
                        audios = audios.unsqueeze(1)
                    
                    # ğŸ”¥ ë°°ì¹˜ ìµœì í™”: ìœ íš¨í•œ guide preset í•„í„°ë§ (ê°„ì†Œí™”)
                    valid_indices = []
                    valid_descriptions = []
                    valid_audios = []
                    valid_guide_presets = []
                    
                    for i, guide_preset in enumerate(guide_presets):
                        # ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ ê²€ì¦ë¨ - ë¹ˆ ë”•ì…”ë„ˆë¦¬ë§Œ í™•ì¸
                        if guide_preset:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ìœ íš¨
                            valid_indices.append(i)
                            valid_descriptions.append(descriptions[i])
                            valid_audios.append(audios[i])
                            valid_guide_presets.append(guide_preset)
                    
                    if len(valid_indices) == 0:
                        continue
                    
                    # ğŸš€ ë°°ì¹˜ ì²˜ë¦¬: ìœ íš¨í•œ í•­ëª©ë“¤ì„ í•œ ë²ˆì— ê²€ì¦
                    try:
                        valid_audio_batch = torch.stack(valid_audios)
                        
                        outputs = self.model(
                            texts=valid_descriptions,
                            audio=valid_audio_batch,
                            use_real_audio=False
                        )
                        
                        if 'preset_params' not in outputs:
                            continue
                            
                        predicted_params = outputs['preset_params']
                        batch_loss = self.compute_batch_guide_loss(predicted_params, valid_guide_presets)
                        
                        total_loss += batch_loss.item()
                        
                        # Progress bar ì—…ë°ì´íŠ¸
                        if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                            pbar.set_postfix({
                                'ValGuideLoss': f'{batch_loss.item():.4f}',
                                'AvgValLoss': f'{total_loss/(batch_idx+1):.4f}',
                                'ValidItems': f'{len(valid_indices)}/{len(descriptions)}',
                                'BatchEff': f'{len(valid_indices)/len(descriptions)*100:.0f}%'
                            })
                    
                    except Exception as e:
                        # Fallback: ê°œë³„ ê²€ì¦
                        batch_loss = 0.0
                        valid_items = 0
                        
                        for desc, audio, guide_preset in zip(valid_descriptions, valid_audios, valid_guide_presets):
                            try:
                                single_audio = audio.unsqueeze(0)
                                
                                outputs = self.model(
                                    texts=[desc],
                                    audio=single_audio,
                                    use_real_audio=False
                                )
                                
                                if 'preset_params' not in outputs:
                                    continue
                                    
                                predicted_params = outputs['preset_params']
                                guide_loss = self.compute_guide_loss(predicted_params, guide_preset)
                                
                                batch_loss += guide_loss.item()
                                valid_items += 1
                                
                            except Exception:
                                continue
                        
                        if valid_items > 0:
                            batch_loss = batch_loss / valid_items
                            total_loss += batch_loss
                        
                        # Progress bar ì—…ë°ì´íŠ¸ (fallback)
                        if self.rank == 0 and hasattr(pbar, 'set_postfix'):
                            pbar.set_postfix({
                                'ValGuideLoss': f'{batch_loss:.4f}' if valid_items > 0 else 'N/A',
                                'ValidItems': f'{valid_items}/{len(descriptions)}',
                                'Mode': 'Fallback'
                            })
                        
                except Exception as e:
                    if self.rank == 0:
                        print(f"âŒ ê²€ì¦ ë°°ì¹˜ {batch_idx} ì‹¤íŒ¨: {e}")
                    continue
        
        # ë¶„ì‚° í›ˆë ¨ì—ì„œ validation loss í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            avg_loss = torch.tensor(total_loss / len(val_loader)).to(self.device)
            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss / self.world_size
            final_val_loss = avg_loss.item()
        else:
            final_val_loss = total_loss / max(1, len(val_loader))
        
        return final_val_loss

    def train(self):
        """ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ - ì‚¬ì „ í›ˆë ¨ + ì¼ë°˜ í›ˆë ¨"""
        if self.rank == 0:
            print("ğŸ¯ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        # 1. ì‚¬ì „ í›ˆë ¨ ì‹¤í–‰ (í™œì„±í™”ëœ ê²½ìš°)
        if self.args.enable_pretrain:
            self.simple_pretrain()
            
            # ì‚¬ì „ í›ˆë ¨ ì™„ë£Œ í›„, ì¼ë°˜ í›ˆë ¨ì—ì„œëŠ” guide preset ë¹„í™œì„±í™”
            if self.rank == 0:
                print("ğŸ”„ ì‚¬ì „ í›ˆë ¨ ì™„ë£Œ - ì¼ë°˜ í›ˆë ¨ì—ì„œ Guide Preset ë¹„í™œì„±í™”")
            self.args.use_guide_presets = False
        
        # 2. ì¼ë°˜ í›ˆë ¨ ì‹œì‘
        if self.rank == 0:
            if self.args.enable_pretrain:
                print("\nğŸ“ ì¼ë°˜ í›ˆë ¨ ì‹œì‘ (Pure Description Training)")
            else:
                print("\nğŸ“ ì¼ë°˜ í›ˆë ¨ ì‹œì‘")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        train_loader, val_loader = self.load_datasets()
        if train_loader is None:
            if self.rank == 0:
                print("âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
        if self.rank == 0:
            os.makedirs(self.args.save_dir, exist_ok=True)
        
        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™”
        if self.world_size > 1:
            dist.barrier()
        
        # í›ˆë ¨ ë£¨í”„
        best_val_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            # í›ˆë ¨
            train_loss = self.train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            val_loss = self.validate(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # ë¡œê¹… (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
            if self.rank == 0:
                training_mode = "Text Descriptions"
                if self.args.use_guide_presets:
                    training_mode = "Guide Presets" if epoch < self.args.guide_epochs else "Text Descriptions"
                
                current_lr = self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.args.learning_rate
                
                # í•™ìŠµ ì§„í–‰ ìƒí™© ë” ìì„¸íˆ ì¶œë ¥
                improvement = ""
                if epoch > 0:
                    if hasattr(self, 'prev_train_loss'):
                        train_diff = train_loss - self.prev_train_loss
                        val_diff = val_loss - self.prev_val_loss
                        improvement = f" (Î”Train: {train_diff:+.6f}, Î”Val: {val_diff:+.6f})"
                
                # í˜„ì¬ ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
                phase_tag = " [Post-Pretrain]" if self.args.enable_pretrain else ""
                print(f"Epoch {epoch+1}/{self.args.num_epochs}: Train={train_loss:.6f}, Val={val_loss:.6f}, LR={current_lr:.1e} ({training_mode}){improvement}{phase_tag}")
                
                # ì´ì „ loss ì €ì¥
                self.prev_train_loss = train_loss
                self.prev_val_loss = val_loss
                
                # í•™ìŠµ ì •ì²´ ê°ì§€
                if epoch > 10:  # 10 ì—í¬í¬ í›„ë¶€í„° í™•ì¸
                    if abs(train_diff) < 1e-6 and abs(val_diff) < 1e-6:
                        print("âš ï¸  í•™ìŠµì´ ì •ì²´ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. Learning rate ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
                
                if self.args.use_wandb:
                    # wandb logging - ê¸€ë¡œë²Œ step ì‚¬ìš© (ë°°ì¹˜ ë‹¨ìœ„)
                    global_step = epoch * len(train_loader) + len(train_loader)  # ì—í¬í¬ ëì˜ step
                    wandb.log({
                        'epoch': epoch + 1,
                        'train_loss_epoch': train_loss,
                        'val_loss_epoch': val_loss,
                        'learning_rate_epoch': self.scheduler.get_last_lr()[0],
                        'training_mode_epoch': training_mode,
                        'is_guide_epoch': self.args.use_guide_presets and epoch < self.args.guide_epochs,
                        'use_guide_presets': self.args.use_guide_presets,
                        'guide_epochs': self.args.guide_epochs if self.args.use_guide_presets else 0,
                        'pretrain_enabled': self.args.enable_pretrain
                    }, step=global_step)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
            
            if (epoch + 1) % self.args.save_every == 0 or is_best:
                self.save_checkpoint(epoch, val_loss, is_best)
        
        if self.rank == 0:
            final_msg = f"âœ… ì „ì²´ í›ˆë ¨ ì™„ë£Œ! Best validation loss: {best_val_loss:.4f}"
            if self.args.enable_pretrain:
                final_msg += " (ì‚¬ì „ í›ˆë ¨ + ì¼ë°˜ í›ˆë ¨)"
            print(final_msg)
            
            # Wandb ì¢…ë£Œ
            if self.args.use_wandb:
                try:
                    wandb.finish()
                except:
                    pass


def train_worker(rank, world_size, args):
    """ë©€í‹°GPU í›ˆë ¨ ì›Œì»¤ í•¨ìˆ˜"""
    try:
        # í›ˆë ¨ ì‹œì‘
        trainer = TrainingManager(args, rank, world_size)
        trainer.train()
    finally:
        # ì •ë¦¬
        if world_size > 1:
            trainer.cleanup_distributed()


def main():
    parser = argparse.ArgumentParser(description='Audio Effect Preset Generation Training')
    
    # ë°ì´í„° ê´€ë ¨
    parser.add_argument('--data_path', type=str, default='/workspace/AudioManipulator',
                       help='ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ')
    parser.add_argument('--sample_rate', type=int, default=44100,
                       help='ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë ˆì´íŠ¸')
    parser.add_argument('--audio_length', type=float, default=5.0,
                       help='ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)')
    parser.add_argument('--max_descriptions', type=int, default=50000,
                       help='ì‚¬ìš©í•  ìµœëŒ€ description ìˆ˜ (0=ëª¨ë‘ ì‚¬ìš©)')
    parser.add_argument('--use_sampled_descriptions', action='store_true', default=False,
                       help='500ê°œ ìƒ˜í”Œ description íŒŒì¼ ì‚¬ìš©')
    
    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument('--text_encoder_type', type=str, default='sentence-transformer-large',
                       choices=['sentence-transformer-mini', 'sentence-transformer-large', 'e5-large', 'clap'],
                       help='ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì¸ì½”ë” íƒ€ì…')
    parser.add_argument('--text_embed_dim', type=int, default=512,
                       help='í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--audio_embed_dim', type=int, default=1024,
                       help='ì˜¤ë””ì˜¤ ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--hidden_dim', type=int, default=256,
                       help='íˆë“  ë ˆì´ì–´ ì°¨ì›')
    
    # í›ˆë ¨ ê´€ë ¨
    parser.add_argument('--batch_size', type=int, default=128,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_epochs', type=int, default=100,
                       help='ì „ì²´ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=1e-2,
                       help='í•™ìŠµë¥ ')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='Weight decay')
    
    # ê°€ì´ë“œ í”„ë¦¬ì…‹ ê´€ë ¨ (ì‚¬ì „ í›ˆë ¨ ë¹„í™œì„±í™”ì‹œì—ë§Œ ì‚¬ìš©)
    parser.add_argument('--use_guide_presets', action='store_true', default=False,
                       help='Fine-tuned guide preset ì‚¬ìš© ì—¬ë¶€ (enable_pretrainê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€)')
    parser.add_argument('--guide_epochs', type=int, default=20,
                       help='Guide presetì„ ì‚¬ìš©í•  ì—í¬í¬ ìˆ˜ (enable_pretrain ë¹„í™œì„±í™”ì‹œì—ë§Œ)')
    parser.add_argument('--guide_weight', type=float, default=0.5,
                       help='Guide lossì˜ ê°€ì¤‘ì¹˜ (enable_pretrain ë¹„í™œì„±í™”ì‹œì—ë§Œ)')
    
    # ì‚¬ì „ í›ˆë ¨ ê´€ë ¨ (ê¶Œì¥: ì‚¬ì „ í›ˆë ¨ ì‚¬ìš©ì‹œ guide preset ì˜µì…˜ë“¤ì€ ë¬´ì‹œë¨)
    parser.add_argument('--enable_pretrain', action='store_true', default=False,
                       help='Guide presetìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ í™œì„±í™” (ì¼ë°˜ í›ˆë ¨ì€ Pure Description)')
    parser.add_argument('--pretrain_epochs', type=int, default=100,
                       help='ì‚¬ì „ í›ˆë ¨ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--pretrain_batch_size', type=int, default=16,
                       help='ì‚¬ì „ í›ˆë ¨ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: ì¼ë°˜ í›ˆë ¨ê³¼ ë™ì¼)')
    parser.add_argument('--pretrain_lr', type=float, default=3e-4,
                       help='ì‚¬ì „ í›ˆë ¨ í•™ìŠµë¥ ')
    
    # ì‹œìŠ¤í…œ ê´€ë ¨
    parser.add_argument('--num_workers', type=int, default=4,
                       help='ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--save_every', type=int, default=10,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°')
    
    # ë¡œê¹…
    parser.add_argument('--use_wandb', action='store_true', default=True,
                       help='Weights & Biases ì‚¬ìš©')
    parser.add_argument('--project_name', type=str, default='audiomanipulator',
                       help='W&B í”„ë¡œì íŠ¸ ì´ë¦„')
    
    # ë©€í‹°GPU ê´€ë ¨
    parser.add_argument('--num_gpus', type=int, default=1,
                       help='ì‚¬ìš©í•  GPU ìˆ˜ (0=CPU only)')
    parser.add_argument('--distributed', action='store_true',
                       help='ë¶„ì‚° í›ˆë ¨ ì‚¬ìš© (DistributedDataParallel)')
    
    args = parser.parse_args()
    
    # GPU ì„¤ì • í™•ì¸
    if args.num_gpus > 0 and not torch.cuda.is_available():
        print("âš ï¸  CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        args.num_gpus = 0
    
    available_gpus = torch.cuda.device_count()
    if args.num_gpus > available_gpus:
        print(f"âš ï¸  ìš”ì²­ëœ GPU ìˆ˜({args.num_gpus})ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜({available_gpus})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
        args.num_gpus = available_gpus
    
    # Argument validation ë° ì¶©ëŒ í•´ê²°
    if args.enable_pretrain and args.use_guide_presets:
        print("âš ï¸  ì‚¬ì „ í›ˆë ¨ í™œì„±í™”ì‹œ ì¼ë°˜ í›ˆë ¨ì—ì„œëŠ” Guide Presetì´ ìë™ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        print("   Guide ê´€ë ¨ arguments (--guide_epochs, --guide_weight)ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.")
        print("   ì‚¬ì „ í›ˆë ¨: Guide Preset â†’ ì¼ë°˜ í›ˆë ¨: Pure Description")
    
    # Weights & Biases ì´ˆê¸°í™” (ìµœì í™”ëœ ì„¤ì •)
    if args.use_wandb:
        try:
            print("ğŸš€ Wandb ì´ˆê¸°í™” ì¤‘...")
            wandb.init(
                project=args.project_name, 
                config=vars(args),  # argsë¥¼ dictë¡œ ë³€í™˜
                name=f"preset-gen-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                tags=['audio-processing', 'clap-loss', 'preset-generation'],
                # ì—…ë¡œë“œ ìµœì í™” ì„¤ì •
                settings=wandb.Settings(
                    _disable_stats=True,  # ì‹œìŠ¤í…œ í†µê³„ ë¹„í™œì„±í™”
                    _disable_meta=True,   # ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ë¹„í™œì„±í™”
                    console="off",        # ì½˜ì†” ë¡œê·¸ ì—…ë¡œë“œ ë¹„í™œì„±í™”
                    code_dir=None,        # ì½”ë“œ ì—…ë¡œë“œ ë¹„í™œì„±í™”
                )
            )
            print("âœ… Wandb ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"ğŸ“Š Project: {args.project_name}")
            print(f"ğŸ”— Run URL: {wandb.run.url if wandb.run else 'N/A'}")
                
        except Exception as e:
            print(f"âš ï¸  Wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("   ë¡œê¹… ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            args.use_wandb = False
    
    print("ğŸµ Audio Effect Preset Generation Training")
    print("=" * 50)
    print(f"ğŸ“ Data path: {args.data_path}")
    print(f"ğŸ¤– Text Encoder: {args.text_encoder_type}")
    print(f"ğŸ›ï¸  Model dimensions: text={args.text_embed_dim}, audio={args.audio_embed_dim}, hidden={args.hidden_dim}")
    print(f"ğŸš€ Training: {args.num_epochs} epochs, batch_size={args.batch_size}, lr={args.learning_rate}")
    
    # í›ˆë ¨ ì „ëµ ì¶œë ¥
    if args.enable_pretrain:
        print(f"ğŸ¯ ì‚¬ì „ í›ˆë ¨ + Pure Description Training")
        print(f"   - ì‚¬ì „ í›ˆë ¨: {args.pretrain_epochs} epochs (Guide Preset), lr={args.pretrain_lr}")
        print(f"   - ì¼ë°˜ í›ˆë ¨: {args.num_epochs} epochs (Pure Description), lr={args.learning_rate}")
    elif args.use_guide_presets:
        print(f"ğŸ¯ Guide Presets + Description Training")
        print(f"   - Guide epochs: {args.guide_epochs}")
        print(f"   - Guide weight: {args.guide_weight}")
        print(f"   - Description epochs: {args.num_epochs - args.guide_epochs}")
    else:
        print(f"ğŸ“ Pure Description Training (no guide presets)")
    
    print(f"ğŸ–¥ï¸  GPU ì„¤ì •: {args.num_gpus} GPUs, distributed={args.distributed}")
    
    # í›ˆë ¨ ì‹œì‘
    if args.num_gpus > 1 and args.distributed:
        # ë¶„ì‚° í›ˆë ¨
        print(f"ğŸš€ ë¶„ì‚° í›ˆë ¨ ì‹œì‘: {args.num_gpus} GPUs")
        mp.spawn(train_worker, args=(args.num_gpus, args), nprocs=args.num_gpus, join=True)
    else:
        # ë‹¨ì¼ GPU ë˜ëŠ” DataParallel í›ˆë ¨
        if args.num_gpus > 1:
            print(f"ğŸš€ DataParallel í›ˆë ¨ ì‹œì‘: {args.num_gpus} GPUs")
        elif args.num_gpus == 1:
            print("ğŸš€ ë‹¨ì¼ GPU í›ˆë ¨ ì‹œì‘")
        else:
            print("ğŸš€ CPU í›ˆë ¨ ì‹œì‘")
        
        trainer = TrainingManager(args, rank=0, world_size=1)
        trainer.train()
        
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì „ ì •ë¦¬
        if args.use_wandb:
            try:
                wandb.finish()
            except:
                pass


if __name__ == "__main__":
    main()
