#!/usr/bin/env python3
"""
ë°°ì¹˜ í¬ê¸°ì— ë”°ë¥¸ ë©”ì¸ í›ˆë ¨ ì‹œê°„ í…ŒìŠ¤íŠ¸
- 16, 32, 64, 128, 256 ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
- ì‹¤ì œ í›ˆë ¨ ì—†ì´ forward passë§Œìœ¼ë¡œ ì‹œê°„ ì¸¡ì •
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ í•¨ê»˜ ì¸¡ì •
"""

import os
import sys
import time
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import traceback

# HuggingFace tokenizers parallelism ê²½ê³  í•´ê²°
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from dynamic_pipeline_factory import DynamicPipelineFactory
from dataset import (
    PureDescriptionDataset,
    create_custom_collate_fn, 
    load_descriptions, 
    split_descriptions
)

def format_time(seconds):
    """ì‹œê°„ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}m"
    else:
        hours = seconds / 3600
        return f"{hours:.1f}h"

def get_gpu_memory_info(device):
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
    if not torch.cuda.is_available():
        return {"allocated": 0, "reserved": 0, "total": 0}
    
    allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB
    reserved = torch.cuda.memory_reserved(device) / 1024**3   # GB
    total = torch.cuda.get_device_properties(device).total_memory / 1024**3  # GB
    
    return {
        "allocated": allocated,
        "reserved": reserved, 
        "total": total,
        "allocated_pct": (allocated/total)*100,
        "reserved_pct": (reserved/total)*100
    }

class BatchSizeTester:
    def __init__(self, data_path='/workspace/AudioManipulator'):
        self.data_path = data_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.descriptions = None
        
        print(f"ğŸš€ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤í„° ì´ˆê¸°í™”")
        print(f"   - Device: {self.device}")
        print(f"   - Data path: {data_path}")
        
    def setup_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("\nğŸ”„ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # CLAP ë¡œë”© ì‹œ ì¶œë ¥ ì–µì œ
        from contextlib import redirect_stdout, redirect_stderr
        
        with open(os.devnull, 'w') as devnull:
            with redirect_stdout(devnull), redirect_stderr(devnull):
                factory = DynamicPipelineFactory()
                
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
                self.model = factory.create_pipeline(
                    encoder_preset='sentence-transformer-large',
                    use_clap=True,
                    backbone_type='residual',
                    decoder_type='parallel',
                    sample_rate=44100,
                    target_params=500000
                )
        
        self.model = self.model.to(self.device)
        self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout ë“± ë¹„í™œì„±í™”)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        
        # ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        if torch.cuda.is_available():
            mem_info = get_gpu_memory_info(self.device)
            print(f"   - GPU ë©”ëª¨ë¦¬ (ëª¨ë¸ ë¡œë“œ í›„): {mem_info['allocated']:.2f}GB allocated, {mem_info['reserved']:.2f}GB reserved")
    
    def load_test_data(self):
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì ì€ ìˆ˜ì˜ descriptionsë§Œ ì‚¬ìš©
        self.descriptions = load_descriptions(
            data_path=self.data_path,
            use_sampled_descriptions=False,  # ê¸°ë³¸ descriptions.txt ì‚¬ìš©
            max_descriptions=1000  # ìµœëŒ€ 1000ê°œë¡œ ì œí•œ
        )
        
        if not self.descriptions:
            raise ValueError("âŒ Description ë¡œë“œ ì‹¤íŒ¨")
        
        # í›ˆë ¨ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (ê²€ì¦ ë°ì´í„°ëŠ” ë¶ˆí•„ìš”)
        train_descriptions, _ = split_descriptions(self.descriptions, train_ratio=0.9)
        self.train_descriptions = train_descriptions[:500]  # 500ê°œë¡œ ì œí•œ
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   - ì‚¬ìš©í•  descriptions: {len(self.train_descriptions)}ê°œ")
        
    def create_test_dataset(self, batch_size):
        """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"""
        dataset = PureDescriptionDataset(
            descriptions=self.train_descriptions,
            audio_dataset_path=os.path.join(self.data_path, 'audio_dataset'),
            sample_rate=44100,
            audio_length=5.0
        )
        
        custom_collate_fn = create_custom_collate_fn(include_guide_preset=False)
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì…”í”Œ ì•ˆí•¨ (ì¼ê´€ì„± ìœ„í•´)
            num_workers=4,
            pin_memory=True,
            collate_fn=custom_collate_fn
        )
        
        return dataloader
    
    def test_batch_size(self, batch_size, num_test_batches=10):
        """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ì‹œê°„ ì¸¡ì •"""
        print(f"\nğŸ”¬ ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            # ë°ì´í„°ë¡œë” ìƒì„±
            dataloader = self.create_test_dataset(batch_size)
            
            # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
            
            # ì›Œë°ì—… (ì²« ë²ˆì§¸ ë°°ì¹˜ëŠ” ì œì™¸)
            warmup_batch = next(iter(dataloader))
            descriptions = warmup_batch['description']
            audios = warmup_batch['audio'].to(self.device, non_blocking=True)
            
            if audios.dim() == 2:
                audios = audios.unsqueeze(1)
            
            # ì›Œë°ì—… forward pass
            with torch.no_grad():
                _ = self.model(texts=descriptions, audio=audios, use_real_audio=False)
            
            # ë©”ëª¨ë¦¬ ì •ë³´ (ì›Œë°ì—… í›„)
            mem_after_warmup = get_gpu_memory_info(self.device) if torch.cuda.is_available() else {}
            
            # ì‹¤ì œ ì‹œê°„ ì¸¡ì •
            batch_times = []
            total_samples = 0
            
            start_time = time.time()
            
            # ì œí•œëœ ìˆ˜ì˜ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
            test_batches = min(num_test_batches, len(dataloader))
            
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= test_batches:
                    break
                
                batch_start = time.time()
                
                descriptions = batch['description']
                audios = batch['audio'].to(self.device, non_blocking=True)
                
                if audios.dim() == 2:
                    audios = audios.unsqueeze(1)
                
                # Forward pass (backwardëŠ” ì œì™¸)
                with torch.no_grad():
                    outputs = self.model(
                        texts=descriptions,
                        audio=audios,
                        use_real_audio=False
                    )
                    
                    # CLAP loss ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ ê³„ì‚°ì€ ì•ˆí•¨)
                    if 'processed_audio' in outputs:
                        processed_audio = outputs['processed_audio']
                        # ê°„ë‹¨í•œ ì—°ì‚°ë§Œ ìˆ˜í–‰ (ì‹¤ì œ CLAP ê³„ì‚° ì—†ì´)
                        _ = torch.mean(processed_audio)
                
                batch_end = time.time()
                batch_time = batch_end - batch_start
                batch_times.append(batch_time)
                total_samples += len(descriptions)
            
            total_time = time.time() - start_time
            
            # í†µê³„ ê³„ì‚°
            avg_batch_time = sum(batch_times) / len(batch_times)
            samples_per_second = total_samples / total_time
            
            # ì „ì²´ ë°ì´í„°ì…‹ ê¸°ì¤€ epoch time ì¶”ì •
            total_dataset_size = len(self.train_descriptions)
            batches_per_epoch = (total_dataset_size + batch_size - 1) // batch_size
            estimated_epoch_time = avg_batch_time * batches_per_epoch
            
            # ë©”ëª¨ë¦¬ ì •ë³´ (í…ŒìŠ¤íŠ¸ í›„)
            mem_after_test = get_gpu_memory_info(self.device) if torch.cuda.is_available() else {}
            peak_memory = torch.cuda.max_memory_allocated(self.device) / 1024**3 if torch.cuda.is_available() else 0
            
            return {
                'batch_size': batch_size,
                'avg_batch_time': avg_batch_time,
                'samples_per_second': samples_per_second,
                'estimated_epoch_time': estimated_epoch_time,
                'total_batches_per_epoch': batches_per_epoch,
                'tested_batches': len(batch_times),
                'total_samples_tested': total_samples,
                'memory_after_warmup': mem_after_warmup,
                'memory_after_test': mem_after_test,
                'peak_memory_gb': peak_memory,
                'success': True
            }
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {
                'batch_size': batch_size,
                'success': False,
                'error': str(e)
            }
    
    def run_comprehensive_test(self):
        """16ë¶€í„° 256ê¹Œì§€ ë°°ì¹˜ í¬ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸ§ª ë°°ì¹˜ í¬ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*80)
        
        # í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸°ë“¤
        batch_sizes = [16, 32, 64, 128, 256]
        results = []
        
        for batch_size in batch_sizes:
            result = self.test_batch_size(batch_size, num_test_batches=10)
            results.append(result)
            
            if result['success']:
                print(f"âœ… ë°°ì¹˜ {batch_size:3d}: "
                      f"{result['avg_batch_time']:.3f}s/batch, "
                      f"{result['samples_per_second']:.1f} samples/s, "
                      f"ì˜ˆìƒ epoch: {format_time(result['estimated_epoch_time'])}")
                
                if torch.cuda.is_available():
                    print(f"      GPU ë©”ëª¨ë¦¬: {result['peak_memory_gb']:.2f}GB peak, "
                          f"{result['memory_after_test']['allocated']:.2f}GB allocated")
            else:
                print(f"âŒ ë°°ì¹˜ {batch_size:3d}: ì‹¤íŒ¨ - {result['error']}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ê²°ê³¼ ìš”ì•½
        self.print_summary(results)
        
        return results
    
    def print_summary(self, results):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        successful_results = [r for r in results if r['success']]
        
        if not successful_results:
            print("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"{'ë°°ì¹˜í¬ê¸°':<8} {'ë°°ì¹˜ì‹œê°„':<10} {'ìƒ˜í”Œ/ì´ˆ':<10} {'Epochì‹œê°„':<12} {'GPUë©”ëª¨ë¦¬':<10} {'íš¨ìœ¨ì„±':<8}")
        print("-" * 70)
        
        # ê¸°ì¤€ ì„±ëŠ¥ (ì²« ë²ˆì§¸ ì„±ê³µí•œ ê²°ê³¼)
        baseline = successful_results[0]
        baseline_efficiency = baseline['samples_per_second']
        
        for result in successful_results:
            if not result['success']:
                continue
                
            batch_size = result['batch_size']
            batch_time = result['avg_batch_time']
            samples_per_sec = result['samples_per_second']
            epoch_time = format_time(result['estimated_epoch_time'])
            
            # GPU ë©”ëª¨ë¦¬
            if torch.cuda.is_available():
                gpu_mem = f"{result['peak_memory_gb']:.1f}GB"
            else:
                gpu_mem = "N/A"
            
            # íš¨ìœ¨ì„± (baseline ëŒ€ë¹„)
            efficiency = (samples_per_sec / baseline_efficiency) * 100
            efficiency_str = f"{efficiency:.0f}%"
            
            print(f"{batch_size:<8} {batch_time:<10.3f} {samples_per_sec:<10.1f} {epoch_time:<12} {gpu_mem:<10} {efficiency_str:<8}")
        
        # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
        best_result = max(successful_results, key=lambda x: x['samples_per_second'])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: ë°°ì¹˜ í¬ê¸° {best_result['batch_size']} "
              f"({best_result['samples_per_second']:.1f} samples/s, "
              f"ì˜ˆìƒ epoch: {format_time(best_result['estimated_epoch_time'])})")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„
        if torch.cuda.is_available():
            print(f"\nğŸ’¾ GPU ë©”ëª¨ë¦¬ ë¶„ì„:")
            for result in successful_results:
                batch_size = result['batch_size']
                peak_mem = result['peak_memory_gb']
                samples_per_gb = result['samples_per_second'] / peak_mem if peak_mem > 0 else 0
                print(f"   ë°°ì¹˜ {batch_size}: {peak_mem:.2f}GB -> {samples_per_gb:.1f} samples/s/GB")
        
        print("\nğŸ“ˆ ê¶Œì¥ì‚¬í•­:")
        print(f"   - ìµœê³  ì²˜ë¦¬ëŸ‰: ë°°ì¹˜ í¬ê¸° {best_result['batch_size']}")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤ ê¶Œì¥
        if torch.cuda.is_available():
            memory_efficient = min(successful_results, 
                                 key=lambda x: x['peak_memory_gb'] / x['samples_per_second'])
            print(f"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: ë°°ì¹˜ í¬ê¸° {memory_efficient['batch_size']}")
        
        print("="*80)

def main():
    parser = argparse.ArgumentParser(description='ë°°ì¹˜ í¬ê¸° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--data_path', type=str, default='/workspace/AudioManipulator',
                       help='ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ')
    parser.add_argument('--num_test_batches', type=int, default=10,
                       help='ê° ë°°ì¹˜ í¬ê¸°ë‹¹ í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ ìˆ˜')
    
    args = parser.parse_args()
    
    try:
        # í…ŒìŠ¤í„° ì´ˆê¸°í™”
        tester = BatchSizeTester(data_path=args.data_path)
        
        # ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
        tester.setup_model()
        tester.load_test_data()
        
        # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = tester.run_comprehensive_test()
        
        print(f"\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
