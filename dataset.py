#!/usr/bin/env python3

import os
import sys
import random
import torch
import librosa
import numpy as np
import importlib.util
import traceback
from pathlib import Path
from torch.utils.data import Dataset


class PretrainDataset(Dataset):
    """ì‚¬ì „ í›ˆë ¨ ì „ìš© ë°ì´í„°ì…‹ - Fine Presetë§Œ ì‚¬ìš©"""
    
    def __init__(self, fine_preset_path, audio_dataset_path, sample_rate=44100, audio_length=5.0):
        self.audio_dataset_path = audio_dataset_path
        self.sample_rate = sample_rate
        self.audio_length = audio_length
        
        # Fine-tuned presetsë§Œ ë¡œë“œ (description ì—†ìŒ)
        self.fine_presets = []
        if fine_preset_path and os.path.exists(fine_preset_path):
            self.fine_presets = self._load_fine_presets(fine_preset_path)
        
        if not self.fine_presets:
            raise ValueError("âŒ ì‚¬ì „ í›ˆë ¨ìš© Fine Presetì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        print(f"ğŸ¯ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì…‹ ì´ˆê¸°í™”:")
        print(f"   - ìœ íš¨í•œ Fine presets ìˆ˜: {len(self.fine_presets)}")
        print(f"   - ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_length}ì´ˆ")
        print(f"   - ëª¨ë“œ: Fine Preset Only (No Descriptions)")
        print(f"   - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(self.fine_presets)} (ìœ íš¨í•œ preset ê°œìˆ˜ì™€ ë™ì¼)")
    
    def _load_fine_presets(self, fine_preset_path):
        """Fine-tuned presets ë¡œë“œ ë° ìœ íš¨ì„± ê²€ì¦"""
        try:
            spec = importlib.util.spec_from_file_location("fine_presets", fine_preset_path)
            fine_presets_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(fine_presets_module)
            
            raw_presets = getattr(fine_presets_module, 'fined_presets', [])
            print(f"ğŸ“¥ {len(raw_presets)}ê°œ raw preset ë¡œë“œë¨")
            
            # ğŸ” ìœ íš¨í•œ presetë§Œ í•„í„°ë§
            valid_presets = []
            invalid_count = 0
            
            for i, preset in enumerate(raw_presets):
                if self._is_valid_preset(preset):
                    valid_presets.append(preset)
                else:
                    invalid_count += 1
                    if invalid_count <= 5:  # ì²˜ìŒ 5ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                        print(f"âš ï¸ ë¬´íš¨í•œ preset (idx {i}): {type(preset)} - {str(preset)[:100]}")
            
            print(f"âœ… {len(valid_presets)}ê°œ ìœ íš¨í•œ preset (ë¬´íš¨: {invalid_count}ê°œ)")
            return valid_presets
            
        except Exception as e:
            print(f"âŒ Fine presets ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _is_valid_preset(self, preset):
        """Preset ìœ íš¨ì„± ê²€ì¦"""
        if not preset or not isinstance(preset, dict):
            return False
        
        # í•„ìˆ˜ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        required_sections = ['eq', 'reverb', 'distortion', 'pitch']
        for section in required_sections:
            if section not in preset:
                return False
            
            section_data = preset[section]
            if not isinstance(section_data, dict):
                return False
        
        # EQ ì„¹ì…˜ ìƒì„¸ ê²€ì¦
        eq_section = preset['eq']
        expected_eq_keys = ['band_1', 'band_2', 'band_3', 'band_4', 'band_5']
        for band in expected_eq_keys:
            if band not in eq_section:
                return False
            band_data = eq_section[band]
            if not isinstance(band_data, dict):
                return False
            # EQ ë°´ë“œ í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì²´í¬
            band_required = ['center_freq', 'gain_db', 'q', 'filter_type']
            for param in band_required:
                if param not in band_data:
                    return False
        
        # Reverb ì„¹ì…˜ ê²€ì¦
        reverb_section = preset['reverb']
        reverb_required = ['room_size', 'pre_delay', 'diffusion', 'damping', 'wet_gain']
        for param in reverb_required:
            if param not in reverb_section:
                return False
        
        # Distortion ì„¹ì…˜ ê²€ì¦
        dist_section = preset['distortion']
        dist_required = ['gain', 'color']
        for param in dist_required:
            if param not in dist_section:
                return False
        
        # Pitch ì„¹ì…˜ ê²€ì¦
        pitch_section = preset['pitch']
        if 'scale' not in pitch_section:
            return False
        
        return True
    
    def _load_audio(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        try:
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            target_length = int(self.sample_rate * self.audio_length)
            
            if len(audio) > target_length:
                start_idx = random.randint(0, len(audio) - target_length)
                audio = audio[start_idx:start_idx + target_length]
            elif len(audio) < target_length:
                audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
            
            return torch.FloatTensor(audio)
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ {audio_path}: {e}")
            return torch.zeros(int(self.sample_rate * self.audio_length))
    
    def __len__(self):
        return len(self.fine_presets)
    
    def __getitem__(self, idx):
        """Fine preset ì•„ì´í…œ ë°˜í™˜ (description ì—†ìŒ) - ì‚¬ì „ ê²€ì¦ëœ preset ì‚¬ìš©"""
        try:
            preset = self.fine_presets[idx]  # ì´ë¯¸ ìœ íš¨ì„± ê²€ì¦ëœ preset
            
            # ëœë¤ ì˜¤ë””ì˜¤ ì„ íƒ (instrumentals í´ë”ì—ì„œ)
            instrumentals_path = os.path.join(self.audio_dataset_path, 'instrumentals')
            audio_files = []
            
            for root, dirs, files in os.walk(instrumentals_path):
                for file in files:
                    if file.lower().endswith(('.wav', '.mp3', '.flac')):
                        audio_files.append(os.path.join(root, file))
            
            if not audio_files:
                print("âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                audio = torch.zeros(int(self.sample_rate * self.audio_length))
            else:
                audio_path = random.choice(audio_files)
                audio = self._load_audio(audio_path)
            
            return {
                'description': f"preset_{idx}",  # ë”ë¯¸ description 
                'audio': audio,
                'guide_preset': preset,  # ì´ë¯¸ ê²€ì¦ëœ preset
                'is_guide': True,  # ì‚¬ì „ í›ˆë ¨ í”Œë˜ê·¸
                'subject': 'instrumental',
                'audio_type': 'instrumental'
            }
            
        except Exception as e:
            print(f"âŒ ì‚¬ì „ í›ˆë ¨ ì•„ì´í…œ ë¡œë“œ ì‹¤íŒ¨ (idx: {idx}): {e}")
            # ë¹ˆ presetì´ ì•„ë‹Œ ê¸°ë³¸ preset êµ¬ì¡° ë°˜í™˜
            default_preset = {
                'eq': {
                    'band_1': {'center_freq': 100, 'gain_db': 0.0, 'q': 1.0, 'filter_type': 'high_pass'},
                    'band_2': {'center_freq': 500, 'gain_db': 0.0, 'q': 1.0, 'filter_type': 'bell'},
                    'band_3': {'center_freq': 2000, 'gain_db': 0.0, 'q': 1.0, 'filter_type': 'bell'},
                    'band_4': {'center_freq': 8000, 'gain_db': 0.0, 'q': 1.0, 'filter_type': 'bell'},
                    'band_5': {'center_freq': 15000, 'gain_db': 0.0, 'q': 1.0, 'filter_type': 'low_pass'}
                },
                'reverb': {'room_size': 5.0, 'pre_delay': 20.0, 'diffusion': 0.7, 'damping': 0.5, 'wet_gain': 0.3},
                'distortion': {'gain': 10.0, 'color': 0.6},
                'pitch': {'scale': 1.0}
            }
            
            return {
                'description': f"preset_{idx}_default",
                'audio': torch.zeros(int(self.sample_rate * self.audio_length)),
                'guide_preset': default_preset,
                'is_guide': True,
                'subject': 'instrumental',
                'audio_type': 'instrumental'
            }


class PresetDataset(Dataset):
    """ì˜¤ë””ì˜¤ ì´í™íŠ¸ í”„ë¦¬ì…‹ ë°ì´í„°ì…‹ - Descriptionê³¼ Guide Preset ì§€ì›"""
    
    def __init__(self, descriptions, audio_dataset_path, use_fine_tuned_presets=False, 
                 fine_preset_path=None, sample_rate=44100, audio_length=5.0):
        self.descriptions = descriptions
        self.audio_dataset_path = audio_dataset_path
        self.sample_rate = sample_rate
        self.audio_length = audio_length
        self.use_fine_tuned_presets = use_fine_tuned_presets
        
        # ì•…ê¸°ì™€ í´ë” ë§¤í•‘
        self.instrument_mapping = {
            'electro guitar': 'Electro_Guitar',
            'acoustic guitar': 'Acoustic_Guitar', 
            'bass guitar': 'Bass_Guitar',
            'piano': 'Piano',
            'keyboard': 'Keyboard',
            'organ': 'Organ',
            'drum set': 'Drum_set',
            'violin': 'Violin',
            'flute': 'flute',
            'saxophone': 'Saxophone',
            'trumpet': 'Trumpet'
        }
        
        # Fine-tuned presets ë¡œë“œ (ê°€ì´ë“œìš©)
        self.fine_presets = []
        if use_fine_tuned_presets and fine_preset_path and os.path.exists(fine_preset_path):
            self.fine_presets = self._load_fine_presets(fine_preset_path)
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   - Description ìˆ˜: {len(self.descriptions)}")
        print(f"   - Fine presets ìˆ˜: {len(self.fine_presets)}")
        print(f"   - ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_length}ì´ˆ")
        print(f"   - Guide Preset ì‚¬ìš©: {'âœ…' if use_fine_tuned_presets else 'âŒ'}")
    
    def _load_fine_presets(self, preset_path):
        """Fine-tuned presets íŒŒì¼ ë¡œë“œ ë° ìœ íš¨ì„± ê²€ì¦"""
        presets = []
        try:
            print(f"ğŸ“– Fine preset íŒŒì¼ ì½ê¸° ì‹œì‘: {preset_path}")
            
            # Python íŒŒì¼ì„ ëª¨ë“ˆë¡œ ë™ì  ë¡œë“œ
            spec = importlib.util.spec_from_file_location("fined_presets", preset_path)
            if spec is None:
                print(f"âŒ ëª¨ë“ˆ ìŠ¤í™ ìƒì„± ì‹¤íŒ¨: {preset_path}")
                return presets
            
            # ëª¨ë“ˆ ìƒì„± ë° ë¡œë“œ
            module = importlib.util.module_from_spec(spec)
            
            # sys.modulesì— ì¶”ê°€ (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)
            module_name = f"fined_presets_{id(module)}"
            sys.modules[module_name] = module
            
            # ì‹¤í–‰
            spec.loader.exec_module(module)
            
            # fined_presets ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            if hasattr(module, 'fined_presets'):
                raw_presets = module.fined_presets
                print(f"ğŸ“¥ {len(raw_presets)}ê°œ raw preset ë¡œë“œë¨")
                
                # ğŸ” ìœ íš¨í•œ presetë§Œ í•„í„°ë§
                valid_presets = []
                invalid_count = 0
                
                for i, preset in enumerate(raw_presets):
                    if self._is_valid_preset(preset):
                        valid_presets.append(preset)
                    else:
                        invalid_count += 1
                        if invalid_count <= 3:  # ì²˜ìŒ 3ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                            print(f"âš ï¸ ë¬´íš¨í•œ preset (idx {i}): {type(preset)}")
                
                presets = valid_presets
                print(f"âœ… {len(presets)}ê°œ ìœ íš¨í•œ preset (ë¬´íš¨: {invalid_count}ê°œ)")
            else:
                print(f"âŒ 'fined_presets' ì†ì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
        except Exception as e:
            print(f"âŒ Fine preset ë¡œë“œ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            
        return presets
    
    def _is_valid_preset(self, preset):
        """Preset ìœ íš¨ì„± ê²€ì¦ (PresetDatasetìš©)"""
        if not preset or not isinstance(preset, dict):
            return False
        
        # í•„ìˆ˜ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        required_sections = ['eq', 'reverb', 'distortion', 'pitch']
        for section in required_sections:
            if section not in preset:
                return False
            
            section_data = preset[section]
            if not isinstance(section_data, dict):
                return False
        
        # EQ ì„¹ì…˜ ìƒì„¸ ê²€ì¦
        eq_section = preset['eq']
        expected_eq_keys = ['band_1', 'band_2', 'band_3', 'band_4', 'band_5']
        for band in expected_eq_keys:
            if band not in eq_section:
                return False
            band_data = eq_section[band]
            if not isinstance(band_data, dict):
                return False
            # EQ ë°´ë“œ í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì²´í¬
            band_required = ['center_freq', 'gain_db', 'q', 'filter_type']
            for param in band_required:
                if param not in band_data:
                    return False
        
        # Reverb ì„¹ì…˜ ê²€ì¦
        reverb_section = preset['reverb']
        reverb_required = ['room_size', 'pre_delay', 'diffusion', 'damping', 'wet_gain']
        for param in reverb_required:
            if param not in reverb_section:
                return False
        
        # Distortion ì„¹ì…˜ ê²€ì¦
        dist_section = preset['distortion']
        dist_required = ['gain', 'color']
        for param in dist_required:
            if param not in dist_section:
                return False
        
        # Pitch ì„¹ì…˜ ê²€ì¦
        pitch_section = preset['pitch']
        if 'scale' not in pitch_section:
            return False
        
        return True
    
    
    def _extract_subject_from_description(self, description):
        """Descriptionì—ì„œ ì£¼ì²´(ì•…ê¸°/ì‚¬ëŒ) ì¶”ì¶œ"""
        description_lower = description.lower()
        
        # ì‚¬ëŒ ì£¼ì²´ í™•ì¸
        if any(word in description_lower for word in ['male', 'female', 'speaks', 'whispers', 'singing', 'says']):
            if 'male' in description_lower:
                return 'male', 'speech'
            elif 'female' in description_lower:
                return 'female', 'speech'
            else:
                return 'neutral', 'speech'
        
        # ì•…ê¸° ì£¼ì²´ í™•ì¸
        for instrument, folder in self.instrument_mapping.items():
            if instrument in description_lower:
                return instrument, 'instrumental'
        
        # ê¸°ë³¸ê°’: í”¼ì•„ë…¸
        return 'piano', 'instrumental'
    
    def _get_random_audio_file(self, subject, audio_type):
        """ì£¼ì²´ì— ë§ëŠ” ëœë¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ"""
        if audio_type == 'speech':
            folder_path = os.path.join(self.audio_dataset_path, 'speech', subject)
        else:  # instrumental
            folder_name = self.instrument_mapping.get(subject, 'Piano')
            folder_path = os.path.join(self.audio_dataset_path, 'instrumentals', folder_name)
        
        if not os.path.exists(folder_path):
            print(f"âš ï¸  í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {folder_path}")
            # ëŒ€ì²´ í´ë” ì‚¬ìš©
            folder_path = os.path.join(self.audio_dataset_path, 'instrumentals', 'Piano')
        
        # WAV, FLAC íŒŒì¼ë“¤ ì°¾ê¸°
        audio_files = []
        for ext in ['*.wav', '*.flac', '*.mp3', '*.m4a']:
            audio_files.extend(list(Path(folder_path).glob(ext)))
        
        if not audio_files:
            print(f"âš ï¸  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {folder_path}")
            return None
        
        return str(random.choice(audio_files))
    
    def _load_audio(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.audio_length)
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            target_length = int(self.sample_rate * self.audio_length)
            if len(audio) < target_length:
                audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
            else:
                audio = audio[:target_length]
            
            return torch.FloatTensor(audio)
        
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ {audio_path}: {e}")
            # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°˜í™˜
            target_length = int(self.sample_rate * self.audio_length)
            return torch.zeros(target_length)
    
    
    def __len__(self):
        return len(self.descriptions)
    
    def __getitem__(self, idx):
        description = self.descriptions[idx]
        
        # ì£¼ì²´ ì¶”ì¶œ
        subject, audio_type = self._extract_subject_from_description(description)
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ ë° ë¡œë“œ
        audio_path = self._get_random_audio_file(subject, audio_type)
        if audio_path is None:
            # ë”ë¯¸ ì˜¤ë””ì˜¤
            target_length = int(self.sample_rate * self.audio_length)
            audio = torch.zeros(target_length)
        else:
            audio = self._load_audio(audio_path)
        
        # Fine preset ê°€ì´ë“œ (ê°„ë‹¨í•œ í• ë‹¹ - ì´ë¯¸ ê²€ì¦ëœ presetë§Œ ì‚¬ìš©)
        guide_preset = {}
        if self.use_fine_tuned_presets and self.fine_presets:
            # ê°„ë‹¨í•œ ëœë¤ í• ë‹¹ (ëª¨ë“  presetì´ ì´ë¯¸ ê²€ì¦ë¨)
            if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ guide preset í• ë‹¹
                guide_preset = random.choice(self.fine_presets)
        
        return {
            'description': description,
            'audio': audio,
            'subject': subject,
            'audio_type': audio_type,
            'guide_preset': guide_preset
        }


class PureDescriptionDataset(Dataset):
    """Pure Description ì „ìš© ë°ì´í„°ì…‹ (Guide Preset ì—†ìŒ)"""
    
    def __init__(self, descriptions, audio_dataset_path, sample_rate=44100, audio_length=5.0):
        self.descriptions = descriptions
        self.audio_dataset_path = audio_dataset_path
        self.sample_rate = sample_rate
        self.audio_length = audio_length
        
        # ì•…ê¸°ì™€ í´ë” ë§¤í•‘
        self.instrument_mapping = {
            'electro guitar': 'Electro_Guitar',
            'acoustic guitar': 'Acoustic_Guitar', 
            'bass guitar': 'Bass_Guitar',
            'piano': 'Piano',
            'keyboard': 'Keyboard',
            'organ': 'Organ',
            'drum set': 'Drum_set',
            'violin': 'Violin',
            'flute': 'flute',
            'saxophone': 'Saxophone',
            'trumpet': 'Trumpet'
        }
        
        print(f"ğŸ“Š Pure Description ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   - Description ìˆ˜: {len(self.descriptions)}")
        print(f"   - ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_length}ì´ˆ")
        print(f"   - Guide Preset: âŒ ë¹„í™œì„±í™” (Pure Description Only)")
    
    def _extract_subject_from_description(self, description):
        """Descriptionì—ì„œ ì£¼ì²´(ì•…ê¸°/ì‚¬ëŒ) ì¶”ì¶œ"""
        description_lower = description.lower()
        
        # ì‚¬ëŒ ì£¼ì²´ í™•ì¸
        if any(word in description_lower for word in ['male', 'female', 'speaks', 'whispers', 'singing', 'says']):
            if 'male' in description_lower:
                return 'male', 'speech'
            elif 'female' in description_lower:
                return 'female', 'speech'
            else:
                return 'neutral', 'speech'
        
        # ì•…ê¸° ì£¼ì²´ í™•ì¸
        for instrument, folder in self.instrument_mapping.items():
            if instrument in description_lower:
                return instrument, 'instrumental'
        
        # ê¸°ë³¸ê°’: í”¼ì•„ë…¸
        return 'piano', 'instrumental'
    
    def _get_random_audio_file(self, subject, audio_type):
        """ì£¼ì²´ì— ë§ëŠ” ëœë¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ"""
        if audio_type == 'speech':
            folder_path = os.path.join(self.audio_dataset_path, 'speech', subject)
        else:  # instrumental
            folder_name = self.instrument_mapping.get(subject, 'Piano')
            folder_path = os.path.join(self.audio_dataset_path, 'instrumentals', folder_name)
        
        if not os.path.exists(folder_path):
            # ëŒ€ì²´ í´ë” ì‚¬ìš©
            folder_path = os.path.join(self.audio_dataset_path, 'instrumentals', 'Piano')
        
        # WAV, FLAC íŒŒì¼ë“¤ ì°¾ê¸°
        audio_files = []
        for ext in ['*.wav', '*.flac', '*.mp3', '*.m4a']:
            audio_files.extend(list(Path(folder_path).glob(ext)))
        
        if not audio_files:
            return None
        
        return str(random.choice(audio_files))
    
    def _load_audio(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.audio_length)
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            target_length = int(self.sample_rate * self.audio_length)
            if len(audio) < target_length:
                audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
            else:
                audio = audio[:target_length]
            
            return torch.FloatTensor(audio)
        
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ {audio_path}: {e}")
            # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°˜í™˜
            target_length = int(self.sample_rate * self.audio_length)
            return torch.zeros(target_length)
    
    def __len__(self):
        return len(self.descriptions)
    
    def __getitem__(self, idx):
        description = self.descriptions[idx]
        
        # ì£¼ì²´ ì¶”ì¶œ
        subject, audio_type = self._extract_subject_from_description(description)
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ ë° ë¡œë“œ
        audio_path = self._get_random_audio_file(subject, audio_type)
        if audio_path is None:
            # ë”ë¯¸ ì˜¤ë””ì˜¤
            target_length = int(self.sample_rate * self.audio_length)
            audio = torch.zeros(target_length)
        else:
            audio = self._load_audio(audio_path)
        
        return {
            'description': description,
            'audio': audio,
            'subject': subject,
            'audio_type': audio_type
        }


def create_custom_collate_fn(include_guide_preset=True):
    """ì»¤ìŠ¤í…€ collate function ìƒì„± - ì‚¬ì „ ê²€ì¦ëœ preset ì‚¬ìš©"""
    
    def collate_fn(batch):
        descriptions = [item['description'] for item in batch]
        audios = torch.stack([item['audio'] for item in batch])
        subjects = [item['subject'] for item in batch]
        audio_types = [item['audio_type'] for item in batch]
        
        result = {
            'description': descriptions,
            'audio': audios,
            'subject': subjects,
            'audio_type': audio_types
        }
        
        if include_guide_preset:
            # ëª¨ë“  presetì€ ì´ë¯¸ datasetì—ì„œ ê²€ì¦ë¨ - ì¶”ê°€ ê²€ì¦ ë¶ˆí•„ìš”
            guide_presets = [item.get('guide_preset', {}) for item in batch]
            result['guide_preset'] = guide_presets
        
        return result
    
    return collate_fn


def load_descriptions(data_path, use_sampled_descriptions=False, max_descriptions=0):
    """Description íŒŒì¼ ë¡œë“œ"""
    descriptions = []
    
    if use_sampled_descriptions:
        desc_file = os.path.join(data_path, 'descriptions', '500_sampled_descriptions.txt')
    else:
        desc_file = os.path.join(data_path, 'descriptions', 'descriptions.txt')
    
    print(f"ğŸ“‚ Description íŒŒì¼ ê²½ë¡œ: {desc_file}")
    
    if os.path.exists(desc_file):
        with open(desc_file, 'r', encoding='utf-8') as f:
            descriptions = [line.strip() for line in f if line.strip()]
    else:
        print(f"âŒ Description íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {desc_file}")
        return []
    
    # ìµœëŒ€ description ìˆ˜ ì œí•œ
    if max_descriptions > 0 and len(descriptions) > max_descriptions:
        print(f"ğŸ“Š Description ìˆ˜ ì œí•œ: {len(descriptions)} â†’ {max_descriptions}")
        random.seed(42)
        descriptions = random.sample(descriptions, max_descriptions)
    
    print(f"ğŸ“š {len(descriptions)}ê°œ description ë¡œë“œë¨")
    if len(descriptions) > 100000:
        print(f"âš ï¸  ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹! í•œ ì—í¬í¬ì— ì•½ {len(descriptions):,}ê°œ description ì‚¬ìš©")
        print(f"   --max_descriptions ì˜µì…˜ìœ¼ë¡œ ì œí•œ ê¶Œì¥ (ì˜ˆ: --max_descriptions 50000)")
    
    return descriptions


def split_descriptions(descriptions, train_ratio=0.8):
    """Descriptionì„ train/validationìœ¼ë¡œ ë¶„í• """
    random.seed(42)
    random.shuffle(descriptions)
    split_idx = int(len(descriptions) * train_ratio)
    train_descriptions = descriptions[:split_idx]
    val_descriptions = descriptions[split_idx:]
    
    return train_descriptions, val_descriptions
